{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05edc1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 16:58:45.143621: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Convolution2D,MaxPooling2D,Flatten\n",
    "from keras.optimizers import Adam\n",
    "import spams\n",
    "import numpy as np\n",
    "import skimage \n",
    "import tensorflow as tf\n",
    "import math as m\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras import layers\n",
    "from mxnet import nd\n",
    "import mxnet as mx\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import losses\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93e147c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749d6b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa4cc516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0:  (array([3, 4, 6, 7, 8], dtype=uint8), array([ 97., 238., 353., 236., 228.], dtype=float32))\n",
      " 1:  (array([ 2,  3,  4,  7, 10], dtype=uint8), array([111., 105., 315., 325., 296.], dtype=float32))\n",
      " 2:  (array([2, 3, 4, 7, 8], dtype=uint8), array([103., 106., 298., 337., 308.], dtype=float32))\n",
      " 3:  (array([1, 2, 3, 5, 6], dtype=uint8), array([111., 116., 116., 309., 500.], dtype=float32))\n",
      " 4:  (array([1, 2, 3, 5, 9], dtype=uint8), array([123., 155., 160., 418., 296.], dtype=float32))\n",
      " 5:  (array([ 1,  3,  7,  8, 10], dtype=uint8), array([106., 120., 330., 313., 283.], dtype=float32))\n",
      " 6:  (array([ 0,  6,  7,  8, 10], dtype=uint8), array([113., 384., 239., 220., 196.], dtype=float32))\n",
      " 7:  (array([0, 2, 6, 7, 9], dtype=uint8), array([141., 118., 402., 264., 227.], dtype=float32))\n",
      " 8:  (array([0, 1, 2, 4, 9], dtype=uint8), array([185., 130., 138., 375., 324.], dtype=float32))\n",
      " 9:  (array([ 1,  4,  7,  8, 10], dtype=uint8), array([ 97., 274., 256., 280., 245.], dtype=float32))\n",
      " 10:  (array([ 1,  3,  6,  7, 10], dtype=uint8), array([ 92.,  96., 429., 268., 267.], dtype=float32))\n",
      " 11:  (array([0, 4, 7, 8, 9], dtype=uint8), array([137., 274., 270., 278., 193.], dtype=float32))\n",
      " 12:  (array([2, 3, 4, 5, 9], dtype=uint8), array([113., 114., 316., 337., 272.], dtype=float32))\n",
      " 13:  (array([ 0,  1,  7,  9, 10], dtype=uint8), array([152., 116., 336., 253., 295.], dtype=float32))\n",
      " 14:  (array([0, 2, 3, 4, 8], dtype=uint8), array([175., 134., 121., 363., 359.], dtype=float32))\n",
      " 15:  (array([2, 3, 4, 8, 9], dtype=uint8), array([111., 115., 323., 350., 253.], dtype=float32))\n",
      " 16:  (array([1, 3, 4, 5, 9], dtype=uint8), array([104., 135., 342., 312., 259.], dtype=float32))\n",
      " 17:  (array([0, 1, 3, 4, 6], dtype=uint8), array([134., 114., 116., 312., 476.], dtype=float32))\n",
      " 18:  (array([ 0,  7,  8,  9, 10], dtype=uint8), array([140., 307., 268., 201., 236.], dtype=float32))\n",
      " 19:  (array([0, 3, 5, 6, 9], dtype=uint8), array([142.,  93., 279., 416., 222.], dtype=float32))\n",
      " 20:  (array([0, 3, 6, 7, 8], dtype=uint8), array([135., 117., 408., 243., 249.], dtype=float32))\n",
      " 21:  (array([1, 2, 3, 6, 9], dtype=uint8), array([137., 116., 131., 517., 251.], dtype=float32))\n",
      " 22:  (array([3, 4, 5, 6, 8], dtype=uint8), array([ 81., 232., 223., 366., 250.], dtype=float32))\n",
      " 23:  (array([ 3,  6,  7,  8, 10], dtype=uint8), array([ 92., 373., 226., 228., 233.], dtype=float32))\n",
      " 24:  (array([ 1,  4,  5,  6, 10], dtype=uint8), array([ 86., 269., 215., 397., 185.], dtype=float32))\n",
      " 25:  (array([ 1,  2,  3,  5, 10], dtype=uint8), array([133., 141., 150., 370., 358.], dtype=float32))\n",
      " 26:  (array([0, 1, 3, 4, 5], dtype=uint8), array([172., 133., 148., 365., 334.], dtype=float32))\n",
      " 27:  (array([ 3,  6,  8,  9, 10], dtype=uint8), array([108., 387., 236., 206., 215.], dtype=float32))\n",
      " 28:  (array([ 1,  3,  4,  5, 10], dtype=uint8), array([115., 121., 338., 307., 271.], dtype=float32))\n",
      " 29:  (array([ 0,  1,  5,  8, 10], dtype=uint8), array([141., 112., 319., 287., 293.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "num_classes = 11\n",
    "# 加载数据''\n",
    "with open('/home/xipeng/FLcode/Organa/my_data_0.5.pkl', 'rb') as f:\n",
    "    data_x , data_y,x_test,y_test  = pickle.load(f)\n",
    "x_train =  data_x \n",
    "y_train =  data_y\n",
    "for i in range(30):\n",
    "    df = pd.DataFrame(data_y[i])\n",
    "    label_counts = df.sum(axis=0)\n",
    "    label_counts = label_counts[label_counts > 0]\n",
    "# 提取标签和数量\n",
    "    labels = np.array(label_counts.index, dtype=np.uint8)\n",
    "    counts = np.array(label_counts.values)\n",
    "# 打印结果\n",
    "    print(f\" {i}: \",(labels, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44eedcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  set_model():\n",
    "    model=Sequential()\n",
    "        # 定义顺序模型           \n",
    "    model.add(Convolution2D(\n",
    "        input_shape = (28,28,1),\n",
    "        filters = 8,\n",
    "        kernel_size = 3,\n",
    "        strides = 1,\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "    ))\n",
    "    # 第一个池化层\n",
    "    model.add(MaxPooling2D(\n",
    "        pool_size = 2,\n",
    "        strides = 2,\n",
    "        padding = 'same',\n",
    "    ))\n",
    "    # 第二个卷积层\n",
    "    model.add(Convolution2D(16,3,strides=1,padding='same',activation='relu'))\n",
    "    # 第二个池化层\n",
    "    model.add(MaxPooling2D(2,2,'same'))                                    \n",
    "                  \n",
    "       # 第二个卷积层      \n",
    "    # 把第三个池化层的输出扁平化为1维\n",
    "    model.add(layers.Flatten())\n",
    "    # 第一个全连接层\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    # Dropout\n",
    "    model.add(Dropout(0.5)) \n",
    "    # 第二个全连接层\n",
    "    model.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "    # 定义优化器\n",
    "    adam = Adam(lr = 5e-4)        \n",
    "        \n",
    "    # 定义优化器,loss function,训练过程中计算准确率\n",
    "    model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "        #print(model_d[i])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ac3347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_square_l2_distance(grads1, grads2):\n",
    "    distance = 0\n",
    "    for g1, g2 in zip(grads1, grads2):\n",
    "        distance += np.linalg.norm(np.array(g1) - np.array(g2)) ** 2\n",
    "    return distance\n",
    "\n",
    "def compute_distances(grads):\n",
    "    n = len(grads)\n",
    "    distances = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            distance = compute_square_l2_distance(grads[i], grads[j])\n",
    "            distances[i, j] = distance\n",
    "            distances[j, i] = distance\n",
    "\n",
    "    return distances\n",
    "def krum(grads, f):\n",
    "    \"\"\"\n",
    "    Krum algorithm for robust gradient aggregation.\n",
    "    \n",
    "    :param grads: List of gradients from participating devices.\n",
    "    :param f: The number of Byzantine (malicious or faulty) devices to tolerate.\n",
    "    :return: The aggregated gradient.\n",
    "    \"\"\"\n",
    "    n = len(grads)\n",
    "    distances = compute_distances(grads)\n",
    "    min_krum_scores = float(\"inf\")\n",
    "    selected_device_index = -1\n",
    "\n",
    "    for i in range(n):\n",
    "        krum_scores = np.partition(distances[i], f)[:f]\n",
    "        krum_score = np.sum(krum_scores)\n",
    "\n",
    "        if krum_score < min_krum_scores:\n",
    "            min_krum_scores = krum_score\n",
    "            selected_device_index = i\n",
    "            print(i)\n",
    "    return grads[selected_device_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d55bd76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3856b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_all(model):\n",
    "    gradients = []\n",
    "    for i in range (n):\n",
    "        for epoch in range(3):\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Get the model's predictions\n",
    "                predictions = model[i](x_train[i])\n",
    "            # Compute the loss using the true labels and the predictions\n",
    "                loss_function = losses.get(model[i].loss)\n",
    "                loss = loss_function(y_train[i], predictions)\n",
    "            \n",
    "        gradients.append(tape.gradient(loss, model[i].trainable_variables))  \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e1534cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sc(gradients,t):\n",
    "    for i in range (t):\n",
    "        for j in range (8):\n",
    "            gradients[i][j] = 1.5*gradients[i][j]\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d578a212",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def noise(gradients,t):\n",
    "    for i in range (t):\n",
    "        for j in range (8):\n",
    "            gradients_np = gradients[i][j].numpy() \n",
    "            noisy_gradients_np = skimage.util.random_noise(gradients_np, mode=\"gaussian\", var=0.2, clip=True)\n",
    "            gradients[i][j] = tf.convert_to_tensor(noisy_gradients_np, dtype=tf.float32)  # Convert back to tensor with dtype=float32\n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da421ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def grad_all(model):\n",
    "#     gradients = []\n",
    "#     for i in range (n):\n",
    "#         for epoch in range(3):\n",
    "#             with tf.GradientTape() as tape:\n",
    "#                 # Get the model's predictions\n",
    "#                 predictions = model[i](x_train[i])\n",
    "#             # Compute the loss using the true labels and the predictions\n",
    "#                 loss_function = losses.get(model[i].loss)\n",
    "#                 loss = loss_function(y_train[i], predictions)\n",
    "            \n",
    "#         gradients.append(tape.gradient(loss, model[i].trainable_variables))  \n",
    "#     return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce84d858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 16:58:49.847272: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2023-06-17 16:58:49.881300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:3b:00.0 name: NVIDIA Tesla V100-PCIE-16GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-06-17 16:58:49.881358: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-06-17 16:58:49.881438: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2023-06-17 16:58:49.881472: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2023-06-17 16:58:49.881507: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2023-06-17 16:58:49.881538: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-06-17 16:58:49.883236: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-06-17 16:58:49.883317: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-06-17 16:58:49.886055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2023-06-17 16:58:49.886811: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-17 16:58:49.899887: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 1700000000 Hz\n",
      "2023-06-17 16:58:49.901045: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56120b92a050 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-17 16:58:49.901081: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-06-17 16:58:50.141857: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56120b996420 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-17 16:58:50.141934: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA Tesla V100-PCIE-16GB, Compute Capability 7.0\n",
      "2023-06-17 16:58:50.149850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:3b:00.0 name: NVIDIA Tesla V100-PCIE-16GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-06-17 16:58:50.149948: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-06-17 16:58:50.150018: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2023-06-17 16:58:50.150062: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2023-06-17 16:58:50.150105: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2023-06-17 16:58:50.150147: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-06-17 16:58:50.150204: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-06-17 16:58:50.150248: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-06-17 16:58:50.155558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2023-06-17 16:58:50.155663: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-06-17 16:58:54.458621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-06-17 16:58:54.458693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
      "2023-06-17 16:58:54.458708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
      "2023-06-17 16:58:54.461781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14216 MB memory) -> physical GPU (device: 0, name: NVIDIA Tesla V100-PCIE-16GB, pci bus id: 0000:3b:00.0, compute capability: 7.0)\n",
      "2023-06-17 16:58:57.122515: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-06-17 16:58:59.548057: W tensorflow/stream_executor/gpu/asm_compiler.cc:81] Running ptxas --version returned 256\n",
      "2023-06-17 16:58:59.775832: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2023-06-17 16:59:02.530688: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "6\n",
      "8\n",
      "556/556 [==============================] - 8s 14ms/step - loss: 2.4033 - accuracy: 0.1146\n",
      "0\n",
      "0\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.3998 - accuracy: 0.1078\n",
      "1\n",
      "0\n",
      "2\n",
      "6\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.3840 - accuracy: 0.1083\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "5\n",
      "6\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.3652 - accuracy: 0.1160\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "5\n",
      "6\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.3484 - accuracy: 0.1347\n",
      "4\n",
      "0\n",
      "1\n",
      "  1/556 [..............................] - ETA: 0s - loss: 2.2981 - accuracy: 0.2188WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0022s vs `on_test_batch_end` time: 0.0049s). Check your callbacks.\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.3388 - accuracy: 0.1696\n",
      "5\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.3343 - accuracy: 0.1855\n",
      "6\n",
      "0\n",
      "1\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.3391 - accuracy: 0.1571\n",
      "7\n",
      "0\n",
      "1\n",
      "5\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.3501 - accuracy: 0.1379\n",
      "8\n",
      "0\n",
      "1\n",
      "5\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.3673 - accuracy: 0.1283\n",
      "9\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.3878 - accuracy: 0.1171\n",
      "10\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4106 - accuracy: 0.1046\n",
      "11\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4354 - accuracy: 0.0998\n",
      "12\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4628 - accuracy: 0.0987\n",
      "13\n",
      "0\n",
      "1\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4982 - accuracy: 0.0984\n",
      "14\n",
      "0\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.5403 - accuracy: 0.0985\n",
      "15\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.5748 - accuracy: 0.0991\n",
      "16\n",
      "0\n",
      "5\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.6096 - accuracy: 0.1021\n",
      "17\n",
      "0\n",
      "5\n",
      "6\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.6294 - accuracy: 0.1112\n",
      "18\n",
      "0\n",
      "5\n",
      "6\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.6349 - accuracy: 0.1283\n",
      "19\n",
      "0\n",
      "5\n",
      "6\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.6277 - accuracy: 0.1554\n",
      "20\n",
      "0\n",
      "6\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.6104 - accuracy: 0.1905\n",
      "21\n",
      "0\n",
      "6\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.5862 - accuracy: 0.2230\n",
      "22\n",
      "0\n",
      "2\n",
      "8\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.5577 - accuracy: 0.2523\n",
      "23\n",
      "0\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.5405 - accuracy: 0.2721\n",
      "24\n",
      "0\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.5328 - accuracy: 0.2783\n",
      "25\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.5304 - accuracy: 0.2838\n",
      "26\n",
      "0\n",
      "1\n",
      "5\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.5320 - accuracy: 0.2866\n",
      "27\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.5371 - accuracy: 0.2896\n",
      "28\n",
      "0\n",
      "1\n",
      "5\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.5460 - accuracy: 0.2897\n",
      "29\n",
      "0\n",
      "1\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.5613 - accuracy: 0.2897\n",
      "30\n",
      "0\n",
      "1\n",
      "2\n",
      "8\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.5666 - accuracy: 0.2917\n",
      "31\n",
      "0\n",
      "1\n",
      "6\n",
      "8\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.5625 - accuracy: 0.2909\n",
      "32\n",
      "0\n",
      "5\n",
      "6\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.5492 - accuracy: 0.2913\n",
      "33\n",
      "0\n",
      "5\n",
      "6\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.5280 - accuracy: 0.2916\n",
      "34\n",
      "0\n",
      "5\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.5125 - accuracy: 0.2924\n",
      "35\n",
      "0\n",
      "6\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4913 - accuracy: 0.2921\n",
      "36\n",
      "0\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4781 - accuracy: 0.2899\n",
      "37\n",
      "0\n",
      "1\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4719 - accuracy: 0.2926\n",
      "38\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4714 - accuracy: 0.2910\n",
      "39\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4765 - accuracy: 0.2918\n",
      "40\n",
      "0\n",
      "1\n",
      "8\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4733 - accuracy: 0.2917\n",
      "41\n",
      "0\n",
      "5\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4743 - accuracy: 0.2928\n",
      "42\n",
      "0\n",
      "5\n",
      "6\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4672 - accuracy: 0.2941\n",
      "43\n",
      "0\n",
      "6\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4530 - accuracy: 0.2946\n",
      "44\n",
      "0\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4454 - accuracy: 0.2978\n",
      "45\n",
      "0\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4438 - accuracy: 0.3023\n",
      "46\n",
      "0\n",
      "2\n",
      "5\n",
      "8\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4354 - accuracy: 0.3046\n",
      "47\n",
      "0\n",
      "1\n",
      "5\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4323 - accuracy: 0.3058\n",
      "48\n",
      "0\n",
      "1\n",
      "5\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4339 - accuracy: 0.3101\n",
      "49\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4444 - accuracy: 0.3125\n",
      "50\n",
      "0\n",
      "1\n",
      "6\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4455 - accuracy: 0.3129\n",
      "51\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4413 - accuracy: 0.3130\n",
      "52\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4326 - accuracy: 0.3135\n",
      "53\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4353 - accuracy: 0.3141\n",
      "54\n",
      "0\n",
      "1\n",
      "8\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4309 - accuracy: 0.3148\n",
      "55\n",
      "0\n",
      "1\n",
      "5\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4322 - accuracy: 0.3154\n",
      "56\n",
      "0\n",
      "1\n",
      "2\n",
      "5\n",
      "8\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4263 - accuracy: 0.3152\n",
      "57\n",
      "0\n",
      "1\n",
      "2\n",
      "5\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4261 - accuracy: 0.3139\n",
      "58\n",
      "0\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4316 - accuracy: 0.3169\n",
      "59\n",
      "0\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4431 - accuracy: 0.3192\n",
      "60\n",
      "0\n",
      "1\n",
      "5\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4591 - accuracy: 0.3213\n",
      "61\n",
      "0\n",
      "1\n",
      "6\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4643 - accuracy: 0.3229\n",
      "62\n",
      "0\n",
      "1\n",
      "6\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4592 - accuracy: 0.3233\n",
      "63\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4668 - accuracy: 0.3232\n",
      "64\n",
      "0\n",
      "1\n",
      "8\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4656 - accuracy: 0.3226\n",
      "65\n",
      "0\n",
      "8\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4569 - accuracy: 0.3226\n",
      "66\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4430 - accuracy: 0.3216\n",
      "67\n",
      "0\n",
      "1\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4358 - accuracy: 0.3221\n",
      "68\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4425 - accuracy: 0.3211\n",
      "69\n",
      "0\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4546 - accuracy: 0.3233\n",
      "70\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4589 - accuracy: 0.3221\n",
      "71\n",
      "0\n",
      "8\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4554 - accuracy: 0.3233\n",
      "72\n",
      "0\n",
      "8\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4457 - accuracy: 0.3253\n",
      "73\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4307 - accuracy: 0.3253\n",
      "74\n",
      "0\n",
      "1\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4226 - accuracy: 0.3263\n",
      "75\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4287 - accuracy: 0.3271\n",
      "76\n",
      "0\n",
      "1\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4394 - accuracy: 0.3282\n",
      "77\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4655 - accuracy: 0.3295\n",
      "78\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4811 - accuracy: 0.3288\n",
      "79\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4854 - accuracy: 0.3304\n",
      "80\n",
      "0\n",
      "8\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4816 - accuracy: 0.3315\n",
      "81\n",
      "0\n",
      "6\n",
      "8\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4718 - accuracy: 0.3301\n",
      "82\n",
      "0\n",
      "1\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4678 - accuracy: 0.3301\n",
      "83\n",
      "0\n",
      "1\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4685 - accuracy: 0.3323\n",
      "84\n",
      "0\n",
      "1\n",
      "6\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4578 - accuracy: 0.3338\n",
      "85\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4642 - accuracy: 0.3341\n",
      "86\n",
      "0\n",
      "1\n",
      "2\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4757 - accuracy: 0.3345\n",
      "87\n",
      "0\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4919 - accuracy: 0.3378\n",
      "88\n",
      "0\n",
      "8\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5003 - accuracy: 0.3370\n",
      "89\n",
      "0\n",
      "1\n",
      "6\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4959 - accuracy: 0.3354\n",
      "90\n",
      "0\n",
      "1\n",
      "5\n",
      "6\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4794 - accuracy: 0.3345\n",
      "91\n",
      "0\n",
      "1\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4695 - accuracy: 0.3369\n",
      "92\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4768 - accuracy: 0.3386\n",
      "93\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5022 - accuracy: 0.3388\n",
      "94\n",
      "0\n",
      "8\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5187 - accuracy: 0.3373\n",
      "95\n",
      "0\n",
      "8\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5249 - accuracy: 0.3375\n",
      "96\n",
      "0\n",
      "8\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5224 - accuracy: 0.3372\n",
      "97\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5107 - accuracy: 0.3340\n",
      "98\n",
      "0\n",
      "3\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5034 - accuracy: 0.3324\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "\n",
    "n= 30;\n",
    "\n",
    "model_list =[]\n",
    "for i in range(30):\n",
    "    model = set_model()\n",
    "    model_list.append(model)\n",
    "\n",
    "global_model = set_model()\n",
    "\n",
    "\n",
    "loss_9=[]\n",
    "t=9\n",
    "for s in range (100):\n",
    "    gradients = grad_all(model_list)\n",
    "    ##\n",
    "    #可以考虑在这中间加入攻击  先看一下gradients的shape，然后看怎么加入攻击\n",
    "    gradients = noise(gradients,t)\n",
    "    ##\n",
    "    selected_gradient = krum(gradients,10) \n",
    "    grads_and_vars = zip(selected_gradient, global_model.trainable_variables)\n",
    "    global_model.optimizer.apply_gradients(grads_and_vars)\n",
    "    loss,accuracy = global_model.evaluate(x_test,y_test,verbose=1)\n",
    "    loss_9.append(loss)\n",
    "    print(s)\n",
    "    for j in range(len(model_list)):\n",
    "        model_list[j].set_weights(global_model.get_weights())\n",
    "scipy.io.savemat('loss_9_1.mat', mdict={'loss_9': loss_9}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5ddadd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a04b50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4696 - accuracy: 0.0429\n",
      "0\n",
      "0\n",
      "1\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4678 - accuracy: 0.0430\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4735 - accuracy: 0.0440\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4903 - accuracy: 0.1024\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5198 - accuracy: 0.2296\n",
      "4\n",
      "0\n",
      "1\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5654 - accuracy: 0.2661\n",
      "5\n",
      "0\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.6302 - accuracy: 0.2714\n",
      "6\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.6760 - accuracy: 0.2592\n",
      "7\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.7023 - accuracy: 0.2237\n",
      "8\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.7104 - accuracy: 0.1881\n",
      "9\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.7022 - accuracy: 0.1812\n",
      "10\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.6804 - accuracy: 0.1824\n",
      "11\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.6483 - accuracy: 0.1848\n",
      "12\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.6100 - accuracy: 0.1866\n",
      "13\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5702 - accuracy: 0.1890\n",
      "14\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5349 - accuracy: 0.1903\n",
      "15\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5096 - accuracy: 0.2031\n",
      "16\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4996 - accuracy: 0.3632\n",
      "17\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5091 - accuracy: 0.3613\n",
      "18\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5394 - accuracy: 0.3377\n",
      "19\n",
      "0\n",
      "3\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5717 - accuracy: 0.3220\n",
      "20\n",
      "0\n",
      "1\n",
      "4\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5953 - accuracy: 0.3173\n",
      "21\n",
      "0\n",
      "1\n",
      "4\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.6062 - accuracy: 0.3242\n",
      "22\n",
      "0\n",
      "1\n",
      "3\n",
      "4\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.6051 - accuracy: 0.3358\n",
      "23\n",
      "0\n",
      "1\n",
      "3\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5952 - accuracy: 0.3484\n",
      "24\n",
      "0\n",
      "3\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5810 - accuracy: 0.3603\n",
      "25\n",
      "0\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5631 - accuracy: 0.3666\n",
      "26\n",
      "0\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5395 - accuracy: 0.3703\n",
      "27\n",
      "0\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5106 - accuracy: 0.3712\n",
      "28\n",
      "0\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4778 - accuracy: 0.3711\n",
      "29\n",
      "0\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4433 - accuracy: 0.3691\n",
      "30\n",
      "0\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4094 - accuracy: 0.3649\n",
      "31\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3879 - accuracy: 0.3610\n",
      "32\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3752 - accuracy: 0.3577\n",
      "33\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3692 - accuracy: 0.3561\n",
      "34\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3693 - accuracy: 0.3555\n",
      "35\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3753 - accuracy: 0.3561\n",
      "36\n",
      "0\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3748 - accuracy: 0.3593\n",
      "37\n",
      "0\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3687 - accuracy: 0.3642\n",
      "38\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3720 - accuracy: 0.3680\n",
      "39\n",
      "0\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3705 - accuracy: 0.3705\n",
      "40\n",
      "0\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3635 - accuracy: 0.3712\n",
      "41\n",
      "0\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3519 - accuracy: 0.3716\n",
      "42\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3491 - accuracy: 0.3714\n",
      "43\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3540 - accuracy: 0.3709\n",
      "44\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3665 - accuracy: 0.3703\n",
      "45\n",
      "0\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3710 - accuracy: 0.3720\n",
      "46\n",
      "0\n",
      "3\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3681 - accuracy: 0.3753\n",
      "47\n",
      "0\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3597 - accuracy: 0.3780\n",
      "48\n",
      "0\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3468 - accuracy: 0.3798\n",
      "49\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3426 - accuracy: 0.3812\n",
      "50\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3460 - accuracy: 0.3816\n",
      "51\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3569 - accuracy: 0.3811\n",
      "52\n",
      "0\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3600 - accuracy: 0.3837\n",
      "53\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3715 - accuracy: 0.3850\n",
      "54\n",
      "0\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3760 - accuracy: 0.3846\n",
      "55\n",
      "0\n",
      "3\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3725 - accuracy: 0.3862\n",
      "56\n",
      "0\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3629 - accuracy: 0.3876\n",
      "57\n",
      "0\n",
      "3\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.3487 - accuracy: 0.3882\n",
      "58\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3447 - accuracy: 0.3878\n",
      "59\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3490 - accuracy: 0.3873\n",
      "60\n",
      "0\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3452 - accuracy: 0.3878\n",
      "61\n",
      "0\n",
      "5\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.3347 - accuracy: 0.3905\n",
      "62\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3333 - accuracy: 0.3914\n",
      "63\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3403 - accuracy: 0.3916\n",
      "64\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3558 - accuracy: 0.3923\n",
      "65\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.3807 - accuracy: 0.3929\n",
      "66\n",
      "0\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3970 - accuracy: 0.3926\n",
      "67\n",
      "0\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4038 - accuracy: 0.3916\n",
      "68\n",
      "0\n",
      "3\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4017 - accuracy: 0.3887\n",
      "69\n",
      "0\n",
      "3\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.3923 - accuracy: 0.3869\n",
      "70\n",
      "0\n",
      "1\n",
      "3\n",
      "4\n",
      "5\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.3748 - accuracy: 0.3876\n",
      "71\n",
      "0\n",
      "1\n",
      "4\n",
      "5\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.3504 - accuracy: 0.3892\n",
      "72\n",
      "0\n",
      "5\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.3217 - accuracy: 0.3919\n",
      "73\n",
      "0\n",
      "5\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.2920 - accuracy: 0.3943\n",
      "74\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.2759 - accuracy: 0.3954\n",
      "75\n",
      "0\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.2600 - accuracy: 0.3978\n",
      "76\n",
      "0\n",
      "3\n",
      "5\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.2479 - accuracy: 0.4012\n",
      "77\n",
      "0\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.2380 - accuracy: 0.4011\n",
      "78\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.2340 - accuracy: 0.4003\n",
      "79\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.2332 - accuracy: 0.4006\n",
      "80\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.2352 - accuracy: 0.4017\n",
      "81\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.2410 - accuracy: 0.4025\n",
      "82\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.2523 - accuracy: 0.4020\n",
      "83\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.2713 - accuracy: 0.3998\n",
      "84\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.3004 - accuracy: 0.3972\n",
      "85\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3415 - accuracy: 0.3960\n",
      "86\n",
      "0\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3772 - accuracy: 0.3945\n",
      "87\n",
      "0\n",
      "1\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4033 - accuracy: 0.3946\n",
      "88\n",
      "0\n",
      "1\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4188 - accuracy: 0.3965\n",
      "89\n",
      "0\n",
      "1\n",
      "3\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4246 - accuracy: 0.3992\n",
      "90\n",
      "0\n",
      "1\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4218 - accuracy: 0.4006\n",
      "91\n",
      "0\n",
      "1\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4111 - accuracy: 0.4005\n",
      "92\n",
      "0\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3944 - accuracy: 0.3994\n",
      "93\n",
      "0\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3737 - accuracy: 0.3982\n",
      "94\n",
      "0\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3522 - accuracy: 0.3986\n",
      "95\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3415 - accuracy: 0.3993\n",
      "96\n",
      "0\n",
      "3\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3298 - accuracy: 0.4012\n",
      "97\n",
      "0\n",
      "3\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3194 - accuracy: 0.4031\n",
      "98\n",
      "0\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3082 - accuracy: 0.4029\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "\n",
    "n= 30;\n",
    "\n",
    "model_list =[]\n",
    "for i in range(30):\n",
    "    model = set_model()\n",
    "    model_list.append(model)\n",
    "\n",
    "global_model = set_model()\n",
    "\n",
    "\n",
    "loss_6=[]\n",
    "t=6\n",
    "for s in range (100):\n",
    "    gradients = grad_all(model_list)\n",
    "    ##\n",
    "    #可以考虑在这中间加入攻击  先看一下gradients的shape，然后看怎么加入攻击\n",
    "    gradients = noise(gradients,t)\n",
    "    ##\n",
    "    selected_gradient = krum(gradients,10) \n",
    "    grads_and_vars = zip(selected_gradient, global_model.trainable_variables)\n",
    "    global_model.optimizer.apply_gradients(grads_and_vars)\n",
    "    loss,accuracy = global_model.evaluate(x_test,y_test,verbose=1)\n",
    "    loss_6.append(loss)\n",
    "    print(s)\n",
    "    for j in range(len(model_list)):\n",
    "        model_list[j].set_weights(global_model.get_weights())\n",
    "scipy.io.savemat('loss_6_1.mat', mdict={'loss_6': loss_6}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "319b7f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4252 - accuracy: 0.1570\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4296 - accuracy: 0.1003\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4468 - accuracy: 0.1052\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4799 - accuracy: 0.1089\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5313 - accuracy: 0.1137\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.6023 - accuracy: 0.1178\n",
      "5\n",
      "0\n",
      "2\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.6939 - accuracy: 0.1261\n",
      "6\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.7714 - accuracy: 0.1357\n",
      "7\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.8325 - accuracy: 0.1469\n",
      "8\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.8747 - accuracy: 0.1574\n",
      "9\n",
      "0\n",
      "6\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.8240 - accuracy: 0.1572\n",
      "10\n",
      "0\n",
      "1\n",
      "6\n",
      "9\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.7632 - accuracy: 0.1899\n",
      "11\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.7110 - accuracy: 0.2191\n",
      "12\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.6659 - accuracy: 0.2374\n",
      "13\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.6272 - accuracy: 0.2469\n",
      "14\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5940 - accuracy: 0.2502\n",
      "15\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5662 - accuracy: 0.2482\n",
      "16\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.5440 - accuracy: 0.2423\n",
      "17\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5258 - accuracy: 0.2362\n",
      "18\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5103 - accuracy: 0.2326\n",
      "19\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4970 - accuracy: 0.2306\n",
      "20\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4854 - accuracy: 0.2298\n",
      "21\n",
      "0\n",
      "2\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4768 - accuracy: 0.2314\n",
      "22\n",
      "0\n",
      "2\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4706 - accuracy: 0.2351\n",
      "23\n",
      "0\n",
      "2\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4664 - accuracy: 0.2409\n",
      "24\n",
      "0\n",
      "2\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4639 - accuracy: 0.2495\n",
      "25\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4617 - accuracy: 0.2563\n",
      "26\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4602 - accuracy: 0.2622\n",
      "27\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4601 - accuracy: 0.2674\n",
      "28\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4620 - accuracy: 0.2739\n",
      "29\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4666 - accuracy: 0.2800\n",
      "30\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4742 - accuracy: 0.2918\n",
      "31\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4854 - accuracy: 0.3206\n",
      "32\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.5006 - accuracy: 0.3499\n",
      "33\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.5200 - accuracy: 0.3756\n",
      "34\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5441 - accuracy: 0.3804\n",
      "35\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.5735 - accuracy: 0.3696\n",
      "36\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5995 - accuracy: 0.3597\n",
      "37\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.6201 - accuracy: 0.3581\n",
      "38\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.6346 - accuracy: 0.3578\n",
      "39\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.6426 - accuracy: 0.3580\n",
      "40\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.6450 - accuracy: 0.3586\n",
      "41\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.6425 - accuracy: 0.3634\n",
      "42\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.6368 - accuracy: 0.3728\n",
      "43\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.6366 - accuracy: 0.3775\n",
      "44\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.6406 - accuracy: 0.3825\n",
      "45\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.6481 - accuracy: 0.3859\n",
      "46\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.6589 - accuracy: 0.3891\n",
      "47\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.6729 - accuracy: 0.3930\n",
      "48\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.6905 - accuracy: 0.3960\n",
      "49\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.7025 - accuracy: 0.3980\n",
      "50\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.7088 - accuracy: 0.4024\n",
      "51\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.7102 - accuracy: 0.4056\n",
      "52\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.7164 - accuracy: 0.4087\n",
      "53\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.7269 - accuracy: 0.4106\n",
      "54\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.7416 - accuracy: 0.4132\n",
      "55\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.7503 - accuracy: 0.4155\n",
      "56\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.7536 - accuracy: 0.4173\n",
      "57\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.7523 - accuracy: 0.4178\n",
      "58\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.7565 - accuracy: 0.4187\n",
      "59\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.7653 - accuracy: 0.4203\n",
      "60\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.7785 - accuracy: 0.4224\n",
      "61\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.7963 - accuracy: 0.4196\n",
      "62\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.8080 - accuracy: 0.4176\n",
      "63\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.8135 - accuracy: 0.4185\n",
      "64\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.8137 - accuracy: 0.4239\n",
      "65\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.8100 - accuracy: 0.4274\n",
      "66\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.8126 - accuracy: 0.4282\n",
      "67\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.8200 - accuracy: 0.4294\n",
      "68\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.8318 - accuracy: 0.4301\n",
      "69\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.8482 - accuracy: 0.4290\n",
      "70\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.8587 - accuracy: 0.4278\n",
      "71\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.8633 - accuracy: 0.4318\n",
      "72\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.8629 - accuracy: 0.4350\n",
      "73\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.8682 - accuracy: 0.4370\n",
      "74\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.8785 - accuracy: 0.4378\n",
      "75\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.8936 - accuracy: 0.4371\n",
      "76\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.9026 - accuracy: 0.4380\n",
      "77\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.9059 - accuracy: 0.4398\n",
      "78\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.9047 - accuracy: 0.4425\n",
      "79\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.9095 - accuracy: 0.4427\n",
      "80\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.9191 - accuracy: 0.4440\n",
      "81\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.9335 - accuracy: 0.4445\n",
      "82\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.9420 - accuracy: 0.4449\n",
      "83\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.9450 - accuracy: 0.4464\n",
      "84\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.9534 - accuracy: 0.4468\n",
      "85\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.9668 - accuracy: 0.4461\n",
      "86\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.9741 - accuracy: 0.4467\n",
      "87\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.9758 - accuracy: 0.4486\n",
      "88\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.9832 - accuracy: 0.4491\n",
      "89\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.9853 - accuracy: 0.4497\n",
      "90\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.9837 - accuracy: 0.4484\n",
      "91\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.9877 - accuracy: 0.4486\n",
      "92\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.9957 - accuracy: 0.4491\n",
      "93\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 3.0078 - accuracy: 0.4514\n",
      "94\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 3.0151 - accuracy: 0.4512\n",
      "95\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 3.0177 - accuracy: 0.4519\n",
      "96\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 3.0170 - accuracy: 0.4490\n",
      "97\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 3.0211 - accuracy: 0.4481\n",
      "98\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 3.0288 - accuracy: 0.4502\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "\n",
    "n= 30;\n",
    "\n",
    "model_list =[]\n",
    "for i in range(30):\n",
    "    model = set_model()\n",
    "    model_list.append(model)\n",
    "\n",
    "global_model = set_model()\n",
    "\n",
    "\n",
    "loss_3=[]\n",
    "t=3\n",
    "for s in range (100):\n",
    "    gradients = grad_all(model_list)\n",
    "    ##\n",
    "    #可以考虑在这中间加入攻击  先看一下gradients的shape，然后看怎么加入攻击\n",
    "    gradients = noise(gradients,t)\n",
    "    ##\n",
    "    selected_gradient = krum(gradients,10) \n",
    "    grads_and_vars = zip(selected_gradient, global_model.trainable_variables)\n",
    "    global_model.optimizer.apply_gradients(grads_and_vars)\n",
    "    loss,accuracy = global_model.evaluate(x_test,y_test,verbose=1)\n",
    "    loss_3.append(loss)\n",
    "    print(s)\n",
    "    for j in range(len(model_list)):\n",
    "        model_list[j].set_weights(global_model.get_weights())\n",
    "scipy.io.savemat('loss_3_1.mat', mdict={'loss_3': loss_3}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82eab0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "6\n",
      "22\n",
      "27\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.9311 - accuracy: 0.4450\n",
      "0\n",
      "0\n",
      "1\n",
      "9\n",
      "22\n",
      "27\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.8167 - accuracy: 0.4075\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "9\n",
      "22\n",
      "27\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.7018 - accuracy: 0.4025\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "9\n",
      "27\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.5847 - accuracy: 0.4006\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "9\n",
      "27\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.4679 - accuracy: 0.3998\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "9\n",
      "27\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3580 - accuracy: 0.3969\n",
      "5\n",
      "0\n",
      "2\n",
      "9\n",
      "27\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.2624 - accuracy: 0.3926\n",
      "6\n",
      "0\n",
      "2\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.1886 - accuracy: 0.3890\n",
      "7\n",
      "0\n",
      "2\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.1321 - accuracy: 0.3868\n",
      "8\n",
      "0\n",
      "2\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.0892 - accuracy: 0.3867\n",
      "9\n",
      "0\n",
      "2\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.0573 - accuracy: 0.3886\n",
      "10\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.0341 - accuracy: 0.3909\n",
      "11\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.0169 - accuracy: 0.3937\n",
      "12\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.0038 - accuracy: 0.3966\n",
      "13\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.9934 - accuracy: 0.3994\n",
      "14\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.9847 - accuracy: 0.4009\n",
      "15\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.9771 - accuracy: 0.4018\n",
      "16\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.9702 - accuracy: 0.4014\n",
      "17\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.9637 - accuracy: 0.4006\n",
      "18\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.9576 - accuracy: 0.3997\n",
      "19\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.9517 - accuracy: 0.3981\n",
      "20\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.9461 - accuracy: 0.3958\n",
      "21\n",
      "0\n",
      "22\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.9319 - accuracy: 0.3928\n",
      "22\n",
      "0\n",
      "22\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.9147 - accuracy: 0.3906\n",
      "23\n",
      "0\n",
      "2\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.9006 - accuracy: 0.3890\n",
      "24\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8893 - accuracy: 0.3888\n",
      "25\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8805 - accuracy: 0.3882\n",
      "26\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8739 - accuracy: 0.3873\n",
      "27\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8692 - accuracy: 0.3873\n",
      "28\n",
      "0\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8655 - accuracy: 0.3877\n",
      "29\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8633 - accuracy: 0.3878\n",
      "30\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8625 - accuracy: 0.3880\n",
      "31\n",
      "0\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8619 - accuracy: 0.3878\n",
      "32\n",
      "0\n",
      "1\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8613 - accuracy: 0.3880\n",
      "33\n",
      "0\n",
      "1\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8607 - accuracy: 0.3888\n",
      "34\n",
      "0\n",
      "1\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8598 - accuracy: 0.3896\n",
      "35\n",
      "0\n",
      "1\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8589 - accuracy: 0.3903\n",
      "36\n",
      "0\n",
      "1\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8579 - accuracy: 0.3918\n",
      "37\n",
      "0\n",
      "1\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8570 - accuracy: 0.3924\n",
      "38\n",
      "0\n",
      "1\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8560 - accuracy: 0.3929\n",
      "39\n",
      "0\n",
      "1\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8552 - accuracy: 0.3928\n",
      "40\n",
      "0\n",
      "1\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8547 - accuracy: 0.3922\n",
      "41\n",
      "0\n",
      "1\n",
      "2\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.8545 - accuracy: 0.3913\n",
      "42\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.8545 - accuracy: 0.3911\n",
      "43\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.8547 - accuracy: 0.3905\n",
      "44\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.8560 - accuracy: 0.3909\n",
      "45\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.8581 - accuracy: 0.3925\n",
      "46\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.8608 - accuracy: 0.3949\n",
      "47\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.8639 - accuracy: 0.3981\n",
      "48\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.8675 - accuracy: 0.4012\n",
      "49\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.8717 - accuracy: 0.4042\n",
      "50\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.8754 - accuracy: 0.4080\n",
      "51\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.8786 - accuracy: 0.4145\n",
      "52\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8814 - accuracy: 0.4193\n",
      "53\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.8840 - accuracy: 0.4250\n",
      "54\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8865 - accuracy: 0.4310\n",
      "55\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8897 - accuracy: 0.4377\n",
      "56\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8934 - accuracy: 0.4415\n",
      "57\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8974 - accuracy: 0.4445\n",
      "58\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.9019 - accuracy: 0.4463\n",
      "59\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.9060 - accuracy: 0.4489\n",
      "60\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.9098 - accuracy: 0.4517\n",
      "61\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.9133 - accuracy: 0.4539\n",
      "62\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.9166 - accuracy: 0.4571\n",
      "63\n",
      "0\n",
      "1\n",
      "22\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.9100 - accuracy: 0.4568\n",
      "64\n",
      "0\n",
      "1\n",
      "22\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8976 - accuracy: 0.4525\n",
      "65\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8876 - accuracy: 0.4474\n",
      "66\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8791 - accuracy: 0.4425\n",
      "67\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8717 - accuracy: 0.4409\n",
      "68\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8651 - accuracy: 0.4406\n",
      "69\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8592 - accuracy: 0.4412\n",
      "70\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8540 - accuracy: 0.4436\n",
      "71\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8495 - accuracy: 0.4479\n",
      "72\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8460 - accuracy: 0.4522\n",
      "73\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8443 - accuracy: 0.4548\n",
      "74\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8435 - accuracy: 0.4596\n",
      "75\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8438 - accuracy: 0.4618\n",
      "76\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8449 - accuracy: 0.4636\n",
      "77\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8466 - accuracy: 0.4635\n",
      "78\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8489 - accuracy: 0.4633\n",
      "79\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8517 - accuracy: 0.4614\n",
      "80\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8553 - accuracy: 0.4589\n",
      "81\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8591 - accuracy: 0.4579\n",
      "82\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8629 - accuracy: 0.4588\n",
      "83\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8666 - accuracy: 0.4606\n",
      "84\n",
      "0\n",
      "1\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8704 - accuracy: 0.4642\n",
      "85\n",
      "0\n",
      "1\n",
      "2\n",
      "17\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8671 - accuracy: 0.4631\n",
      "86\n",
      "0\n",
      "1\n",
      "2\n",
      "17\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8597 - accuracy: 0.4592\n",
      "87\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8537 - accuracy: 0.4573\n",
      "88\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8487 - accuracy: 0.4576\n",
      "89\n",
      "0\n",
      "1\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8444 - accuracy: 0.4580\n",
      "90\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8409 - accuracy: 0.4593\n",
      "91\n",
      "0\n",
      "1\n",
      "2\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8379 - accuracy: 0.4618\n",
      "92\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8357 - accuracy: 0.4641\n",
      "93\n",
      "0\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8344 - accuracy: 0.4673\n",
      "94\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8340 - accuracy: 0.4690\n",
      "95\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8341 - accuracy: 0.4692\n",
      "96\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8347 - accuracy: 0.4698\n",
      "97\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8357 - accuracy: 0.4705\n",
      "98\n",
      "0\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8375 - accuracy: 0.4714\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "for s in range (100):\n",
    "    gradients = grad_all(model_list)\n",
    "    ##\n",
    "    #可以考虑在这中间加入攻击  先看一下gradients的shape，然后看怎么加入攻击\n",
    "    gradients = noise(gradients,t)\n",
    "    ##\n",
    "    selected_gradient = krum(gradients,20) \n",
    "    grads_and_vars = zip(selected_gradient, global_model.trainable_variables)\n",
    "    global_model.optimizer.apply_gradients(grads_and_vars)\n",
    "    loss,accuracy = global_model.evaluate(x_test,y_test,verbose=1)\n",
    "    loss_3.append(loss)\n",
    "    print(s)\n",
    "    for j in range(len(model_list)):\n",
    "        model_list[j].set_weights(global_model.get_weights())\n",
    "scipy.io.savemat('loss_3_1.mat', mdict={'loss_3': loss_3}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c9e563",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
