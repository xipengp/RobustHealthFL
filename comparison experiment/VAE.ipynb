{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "992299de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-06 19:49:31.851991: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Convolution2D,MaxPooling2D,Flatten\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import mxnet as mx\n",
    "from tensorflow import keras\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "# 定义输入层\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e817fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a713cfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 0: 1152 samples\n",
      "Device 0:  (array([ 2,  3,  5,  8, 10], dtype=uint8), array([114, 113, 300, 331, 294]))\n",
      "Device 1: 1152 samples\n",
      "Device 1:  (array([2, 6, 7, 8, 9], dtype=uint8), array([ 93, 369, 249, 243, 198]))\n",
      "Device 2: 1152 samples\n",
      "Device 2:  (array([1, 5, 7, 8, 9], dtype=uint8), array([115, 287, 273, 257, 220]))\n",
      "Device 3: 1152 samples\n",
      "Device 3:  (array([ 0,  3,  7,  8, 10], dtype=uint8), array([155, 113, 299, 328, 257]))\n",
      "Device 4: 1152 samples\n",
      "Device 4:  (array([3, 5, 6, 7, 9], dtype=uint8), array([ 98, 218, 399, 229, 208]))\n",
      "Device 5: 1152 samples\n",
      "Device 5:  (array([ 1,  4,  5,  7, 10], dtype=uint8), array([110, 278, 252, 271, 241]))\n",
      "Device 6: 1152 samples\n",
      "Device 6:  (array([2, 3, 5, 6, 9], dtype=uint8), array([106, 103, 290, 442, 211]))\n",
      "Device 7: 1152 samples\n",
      "Device 7:  (array([1, 3, 4, 8, 9], dtype=uint8), array([133, 127, 287, 341, 264]))\n",
      "Device 8: 1152 samples\n",
      "Device 8:  (array([0, 2, 3, 5, 9], dtype=uint8), array([192, 119, 139, 391, 311]))\n",
      "Device 9: 1152 samples\n",
      "Device 9:  (array([ 0,  2,  8,  9, 10], dtype=uint8), array([149, 128, 313, 263, 299]))\n",
      "Device 10: 1152 samples\n",
      "Device 10:  (array([ 1,  3,  6,  9, 10], dtype=uint8), array([106, 109, 461, 223, 253]))\n",
      "Device 11: 1152 samples\n",
      "Device 11:  (array([ 0,  6,  8,  9, 10], dtype=uint8), array([ 98, 376, 244, 218, 216]))\n",
      "Device 12: 1152 samples\n",
      "Device 12:  (array([ 0,  1,  2,  5, 10], dtype=uint8), array([160, 132, 138, 374, 348]))\n",
      "Device 13: 1152 samples\n",
      "Device 13:  (array([ 0,  1,  2,  6, 10], dtype=uint8), array([149, 117, 115, 488, 283]))\n",
      "Device 14: 1152 samples\n",
      "Device 14:  (array([1, 5, 6, 8, 9], dtype=uint8), array([ 78, 237, 381, 255, 201]))\n",
      "Device 15: 1152 samples\n",
      "Device 15:  (array([0, 3, 7, 8, 9], dtype=uint8), array([164, 104, 301, 318, 265]))\n",
      "Device 16: 1152 samples\n",
      "Device 16:  (array([ 4,  7,  8,  9, 10], dtype=uint8), array([263, 254, 232, 186, 217]))\n",
      "Device 17: 1152 samples\n",
      "Device 17:  (array([ 3,  4,  8,  9, 10], dtype=uint8), array([121, 271, 279, 219, 262]))\n",
      "Device 18: 1152 samples\n",
      "Device 18:  (array([0, 1, 2, 5, 7], dtype=uint8), array([166, 139, 111, 344, 392]))\n",
      "Device 19: 1152 samples\n",
      "Device 19:  (array([ 0,  1,  7,  8, 10], dtype=uint8), array([150, 110, 296, 309, 287]))\n",
      "Device 20: 1152 samples\n",
      "Device 20:  (array([0, 1, 2, 4, 8], dtype=uint8), array([167, 134, 107, 386, 358]))\n",
      "Device 21: 1152 samples\n",
      "Device 21:  (array([3, 4, 5, 6, 7], dtype=uint8), array([ 98, 249, 216, 358, 231]))\n",
      "Device 22: 1152 samples\n",
      "Device 22:  (array([1, 2, 4, 6, 9], dtype=uint8), array([102, 110, 285, 427, 228]))\n",
      "Device 23: 1152 samples\n",
      "Device 23:  (array([ 3,  4,  5,  6, 10], dtype=uint8), array([ 91, 215, 248, 373, 225]))\n",
      "Device 24: 1152 samples\n",
      "Device 24:  (array([0, 1, 2, 5, 6], dtype=uint8), array([147, 116,  96, 294, 499]))\n",
      "Device 25: 1152 samples\n",
      "Device 25:  (array([1, 2, 3, 7, 8], dtype=uint8), array([130, 120, 147, 390, 365]))\n",
      "Device 26: 1152 samples\n",
      "Device 26:  (array([ 2,  3,  4,  7, 10], dtype=uint8), array([117, 104, 346, 307, 278]))\n",
      "Device 27: 1152 samples\n",
      "Device 27:  (array([0, 1, 4, 7, 9], dtype=uint8), array([169, 108, 334, 306, 235]))\n",
      "Device 28: 1152 samples\n",
      "Device 28:  (array([ 1,  2,  3,  4, 10], dtype=uint8), array([152, 142, 151, 374, 333]))\n",
      "Device 29: 1152 samples\n",
      "Device 29:  (array([3, 4, 6, 7, 9], dtype=uint8), array([ 91, 260, 391, 239, 171]))\n"
     ]
    }
   ],
   "source": [
    "data = np.load('/home/xipeng/FLcode/organamnist.npz')\n",
    "data.files\n",
    "\n",
    "x_train = data[\"train_images\"]\n",
    "y_train = data[\"train_labels\"]\n",
    "x_test = data[\"test_images\"]\n",
    "y_test = data[\"test_labels\"]\n",
    "\n",
    "# 为图像数据添加通道维度\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# 归一化图像数据\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "# 设备数量和IID程度\n",
    "num_devices = 30\n",
    "iid_ratio = 0.5\n",
    "\n",
    "# 计算每个设备应包含的类别数量\n",
    "num_classes = len(np.unique(y_train))\n",
    "samples_per_device = len(x_train) // num_devices\n",
    "selected_classes_per_device = int(num_classes * iid_ratio)\n",
    "\n",
    "# 分配数据\n",
    "device_data = [[] for _ in range(num_devices)]\n",
    "\n",
    "# 为每个设备分配数据\n",
    "for device_id in range(num_devices):\n",
    "    # 随机选择类别\n",
    "    random_classes = list(range(num_classes))\n",
    "    random.shuffle(random_classes)\n",
    "    selected_classes = random_classes[:selected_classes_per_device]\n",
    "\n",
    "    # 获取已选类别的索引\n",
    "    selected_indices = []\n",
    "    for cls in selected_classes:\n",
    "        class_indices = np.where(np.array(y_train) == cls)[0]\n",
    "        selected_indices.extend(class_indices)\n",
    "\n",
    "    # 在已选类别的索引中进行随机抽样\n",
    "    random_selected_indices = random.sample(selected_indices, samples_per_device)\n",
    "    device_data[device_id] = [(x_train[i], y_train[i]) for i in random_selected_indices]\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "# 检查每个设备上的样本数量\n",
    "for i in range(num_devices):\n",
    "    print(f\"Device {i}: {len(device_data[i])} samples\")\n",
    "    print(f\"Device {i}: \", np.unique(np.array([label for _, label in device_data[i]]), return_counts=True))  \n",
    "data_x = [np.array([sample for sample, _ in device_data[i]]) for i in range(num_devices)]\n",
    "data_y = [np.array([label for _, label in device_data[i]]) for i in range(num_devices)]\n",
    "data_y = keras.utils.to_categorical(data_y, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "x_train = data_x\n",
    "y_train = data_y\n",
    "\n",
    "data_x_1 = data_x[0]\n",
    "\n",
    "data_y_1 = data_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78d4fe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_partial_backdoor(images, labels_onehot, backdoor_label=8, trigger_size=3, backdoor_ratio=0.5):\n",
    "    backdoor_images = []\n",
    "    backdoor_labels_onehot = []\n",
    "\n",
    "    num_backdoor_samples = int(len(images) * backdoor_ratio)\n",
    "    backdoor_indices = np.random.choice(len(images), size=num_backdoor_samples, replace=False)\n",
    "\n",
    "    for idx, (image, label_onehot) in enumerate(zip(images, labels_onehot)):\n",
    "        backdoor_image = np.copy(image)\n",
    "        backdoor_label_onehot = np.copy(label_onehot)\n",
    "        \n",
    "        if idx in backdoor_indices:\n",
    "            backdoor_image[-trigger_size:, -trigger_size:] = 1.0\n",
    "            backdoor_label_onehot = np.zeros_like(label_onehot)\n",
    "            backdoor_label_onehot[backdoor_label] = 1\n",
    "            \n",
    "        backdoor_images.append(backdoor_image)\n",
    "        backdoor_labels_onehot.append(backdoor_label_onehot)\n",
    "\n",
    "    return np.array(backdoor_images), np.array(backdoor_labels_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4376be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_data(x_train,y_train,n):   \n",
    "    data_x = {}\n",
    "    data_y = {}\n",
    "    t = int(60000/n)\n",
    "    print(t)\n",
    "    print(n)\n",
    "    for i in range(n):\n",
    "        data_x[i] = x_train[t*i:t+t*i,:,:]\n",
    "\n",
    "        data_y[i] = y_train[t*i:t+t*i,:]\n",
    "\n",
    "    print(data_x[n-1].shape)\n",
    "    print(data_y[n-1].shape)\n",
    "    return data_x,data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03387894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  set_model():\n",
    "    model=Sequential()\n",
    "        # 定义顺序模型           \n",
    "    model.add(Convolution2D(\n",
    "        input_shape = (28,28,1),\n",
    "        filters = 8,\n",
    "        kernel_size = 3,\n",
    "        strides = 1,\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "    ))\n",
    "    # 第一个池化层\n",
    "    model.add(MaxPooling2D(\n",
    "        pool_size = 2,\n",
    "        strides = 2,\n",
    "        padding = 'same',\n",
    "    ))\n",
    "    # 第二个卷积层\n",
    "    model.add(Convolution2D(16,3,strides=1,padding='same',activation='relu'))\n",
    "    # 第二个池化层\n",
    "    model.add(MaxPooling2D(2,2,'same'))                                    \n",
    "                  \n",
    "       # 第二个卷积层      \n",
    "    # 把第三个池化层的输出扁平化为1维\n",
    "    model.add(layers.Flatten())\n",
    "    # 第一个全连接层\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    # Dropout\n",
    "    model.add(Dropout(0.5)) \n",
    "    # 第二个全连接层\n",
    "    model.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "    # 定义优化器\n",
    "    adam = Adam(lr = 5e-4)        \n",
    "        \n",
    "    # 定义优化器,loss function,训练过程中计算准确率\n",
    "    model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "        #print(model_d[i])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e102893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f0a97fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_x(weights):\n",
    "    extracted_weights = []\n",
    "    for i in range(len(weights)):\n",
    "            # 卷积层的权重参数和偏置参数\n",
    "        if weights[i].ndim == 4:\n",
    "            conv_weights = weights[i][:, :, 0, 0]\n",
    "            # 将卷积核转换为列向量\n",
    "            conv_weights_col = np.reshape(conv_weights, (-1,))\n",
    "            extracted_weights.append(conv_weights_col)\n",
    "            # 全连接层的权重参数和偏置参数\n",
    "        elif weights[i].ndim == 2:\n",
    "            array = weights[i].flatten()\n",
    "            extracted_weights.append(array[:20])\n",
    "        elif weights[i].ndim == 1:\n",
    "            if len(weights[i]) >= 20:\n",
    "                extracted_weights.append(weights[i][0:20])\n",
    "            else:\n",
    "                extracted_weights.append(weights[i])\n",
    "    return extracted_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "105a04d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-06 19:49:38.484354: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2023-07-06 19:49:38.519935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:3b:00.0 name: NVIDIA Tesla V100-PCIE-16GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-07-06 19:49:38.519998: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-07-06 19:49:38.520076: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2023-07-06 19:49:38.520108: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2023-07-06 19:49:38.520141: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2023-07-06 19:49:38.520171: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-07-06 19:49:38.521540: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-07-06 19:49:38.521641: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-07-06 19:49:38.524354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2023-07-06 19:49:38.525230: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-06 19:49:38.545650: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 1700000000 Hz\n",
      "2023-07-06 19:49:38.547423: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558b51a3c460 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-07-06 19:49:38.547468: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-07-06 19:49:38.797102: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558b51aa8980 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-07-06 19:49:38.797187: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA Tesla V100-PCIE-16GB, Compute Capability 7.0\n",
      "2023-07-06 19:49:38.800588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:3b:00.0 name: NVIDIA Tesla V100-PCIE-16GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-07-06 19:49:38.800675: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-07-06 19:49:38.800744: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2023-07-06 19:49:38.800788: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2023-07-06 19:49:38.800833: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2023-07-06 19:49:38.800875: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-07-06 19:49:38.800937: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-07-06 19:49:38.800979: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-07-06 19:49:38.806288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2023-07-06 19:49:38.806409: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-07-06 19:49:43.273133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-07-06 19:49:43.273217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
      "2023-07-06 19:49:43.273234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
      "2023-07-06 19:49:43.281801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14216 MB memory) -> physical GPU (device: 0, name: NVIDIA Tesla V100-PCIE-16GB, pci bus id: 0000:3b:00.0, compute capability: 7.0)\n",
      "2023-07-06 19:49:44.926704: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2023-07-06 19:49:45.332407: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-07-06 19:49:47.573146: W tensorflow/stream_executor/gpu/asm_compiler.cc:81] Running ptxas --version returned 256\n",
      "2023-07-06 19:49:47.801627: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_test = set_model()\n",
    "model_test_1 = set_model()\n",
    "model_test_2 = set_model()\n",
    "results = []\n",
    "results2 = []\n",
    "for i in range(200):\n",
    "    model_test.fit(x_test,y_test,batch_size=16,epochs=1,verbose=0)\n",
    "    weights = model_test.get_weights()\n",
    "    extracted_vectors_1 = e_x(weights)\n",
    "    result_all = np.concatenate([extracted_vectors_1[0], extracted_vectors_1[1], extracted_vectors_1[2], extracted_vectors_1[3], extracted_vectors_1[4],extracted_vectors_1[5], extracted_vectors_1[6], extracted_vectors_1[7]])               \n",
    "    results.append(result_all)\n",
    "matrix = np.vstack(results)\n",
    "\n",
    "for i in range(100):\n",
    "    model_test_2.fit(x_test,y_test,batch_size=16,epochs=2,verbose=0)\n",
    "    weights = model_test.get_weights()\n",
    "    extracted_vectors_1 = e_x(weights)\n",
    "    result_all = np.concatenate([extracted_vectors_1[0], extracted_vectors_1[1], extracted_vectors_1[2], extracted_vectors_1[3], extracted_vectors_1[4],extracted_vectors_1[5], extracted_vectors_1[6], extracted_vectors_1[7]])               \n",
    "    results.append(result_all)\n",
    "matrix_2 = np.vstack(results)\n",
    "\n",
    "\n",
    "for i in range(50):\n",
    "    model_test_1.fit(x_test,y_test,batch_size=16,epochs=1,verbose=0)\n",
    "    weights = model_test_1.get_weights()\n",
    "    extracted_vectors_1 = e_x(weights)\n",
    "    result_all2 = np.concatenate([extracted_vectors_1[0], extracted_vectors_1[1], extracted_vectors_1[2], extracted_vectors_1[3], extracted_vectors_1[4],extracted_vectors_1[5], extracted_vectors_1[6], extracted_vectors_1[7]])               \n",
    "    results2.append(result_all2)\n",
    "matrix2 = np.vstack(results2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bddebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a12567e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 113)]             0         \n",
      "_________________________________________________________________\n",
      "functional_1 (Functional)    (None, 100)               357600    \n",
      "_________________________________________________________________\n",
      "functional_3 (Functional)    (None, 113)               357613    \n",
      "=================================================================\n",
      "Total params: 715,213\n",
      "Trainable params: 715,213\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def build_encoder(input_dim, hidden_units, latent_dim):\n",
    "    encoder_input = Input(shape=(input_dim,))\n",
    "    encoder_hidden_1 = Dense(hidden_units, activation='relu')(encoder_input)\n",
    "    encoder_hidden_2 = Dense(hidden_units, activation='relu')(encoder_hidden_1)\n",
    "    encoder_output = Dense(latent_dim, activation='relu')(encoder_hidden_2)\n",
    "    encoder = Model(encoder_input, encoder_output)\n",
    "    return encoder\n",
    "\n",
    "def build_decoder(latent_dim, hidden_units, output_dim):\n",
    "    decoder_input = Input(shape=(latent_dim,))\n",
    "    decoder_hidden_1 = Dense(hidden_units, activation='relu')(decoder_input)\n",
    "    decoder_hidden_2 = Dense(hidden_units, activation='relu')(decoder_hidden_1)\n",
    "    decoder_output = Dense(output_dim, activation='linear')(decoder_hidden_2)\n",
    "    decoder = Model(decoder_input, decoder_output)\n",
    "    return decoder\n",
    "\n",
    "def build_autoencoder(input_dim, hidden_units, latent_dim):\n",
    "    encoder = build_encoder(input_dim, hidden_units, latent_dim)\n",
    "    decoder = build_decoder(latent_dim, hidden_units, input_dim)\n",
    "\n",
    "    autoencoder_input = Input(shape=(input_dim,))\n",
    "    encoded = encoder(autoencoder_input)\n",
    "    decoded = decoder(encoded)\n",
    "    autoencoder = Model(autoencoder_input, decoded)\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "# 设置参数\n",
    "input_dim = 113\n",
    "latent_dim = 100\n",
    "hidden_units = 500\n",
    "\n",
    "# 构建自编码器\n",
    "autoencoder = build_autoencoder(input_dim, hidden_units, latent_dim)\n",
    "\n",
    "# 编译模型\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "print(autoencoder.summary())\n",
    "\n",
    "# 设置参数\n",
    "input_dim = 113\n",
    "latent_dim = 100\n",
    "hidden_units = 500\n",
    "\n",
    "# 构建自编码器\n",
    "autoencoder = build_autoencoder(input_dim, hidden_units, latent_dim)\n",
    "\n",
    "# 编译模型\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=2e-5), loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80feea09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0970 - val_loss: 0.0497\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0965 - val_loss: 0.0497\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0960 - val_loss: 0.0497\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0955 - val_loss: 0.0497\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0950 - val_loss: 0.0496\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0945 - val_loss: 0.0496\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0941 - val_loss: 0.0496\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0936 - val_loss: 0.0496\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0932 - val_loss: 0.0496\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0927 - val_loss: 0.0495\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0922 - val_loss: 0.0495\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0918 - val_loss: 0.0495\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0913 - val_loss: 0.0495\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0909 - val_loss: 0.0495\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0904 - val_loss: 0.0495\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0900 - val_loss: 0.0495\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0895 - val_loss: 0.0495\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0891 - val_loss: 0.0494\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0886 - val_loss: 0.0494\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0882 - val_loss: 0.0494\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0877 - val_loss: 0.0494\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0872 - val_loss: 0.0494\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0868 - val_loss: 0.0494\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0863 - val_loss: 0.0494\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0858 - val_loss: 0.0494\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0853 - val_loss: 0.0494\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0848 - val_loss: 0.0494\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0842 - val_loss: 0.0494\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0837 - val_loss: 0.0494\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0832 - val_loss: 0.0494\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0826 - val_loss: 0.0494\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0820 - val_loss: 0.0494\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0815 - val_loss: 0.0494\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0809 - val_loss: 0.0494\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0803 - val_loss: 0.0494\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0796 - val_loss: 0.0494\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0790 - val_loss: 0.0494\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0784 - val_loss: 0.0494\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0777 - val_loss: 0.0494\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0771 - val_loss: 0.0494\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0764 - val_loss: 0.0494\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0757 - val_loss: 0.0494\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0750 - val_loss: 0.0494\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0743 - val_loss: 0.0494\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0735 - val_loss: 0.0494\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0728 - val_loss: 0.0495\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0720 - val_loss: 0.0495\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0712 - val_loss: 0.0495\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0704 - val_loss: 0.0495\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0696 - val_loss: 0.0495\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train_e = matrix  # 输入数据 (shape: (num_samples, input_dim))\n",
    "x_test_e = matrix2   # 测试数据 (shape: (num_test_samples, input_dim))\n",
    "# 训练模型\n",
    "autoencoder.fit(x_train_e, x_train_e,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test_e, x_test_e))\n",
    "\n",
    "reconstructed_test_1 = autoencoder.predict(x_train_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41a68425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#所有的客户端训练一遍\n",
    "def fit_allmodel(model_d,att_k):\n",
    "    \n",
    "    weights_1 = {}\n",
    "    for i in range(len(model_d)):\n",
    "    # 训练模型\n",
    "        model_d[i].fit(data_x[i],data_y[i],batch_size=16,epochs=3,verbose=0)\n",
    "        loss,accuracy = model_d[i].evaluate(x_test,y_test,verbose=0)         \n",
    "    for i in range(len(model_d)): \n",
    "        weights_1[i] = model_d[i].get_weights() \n",
    "    #对前5个数据进行          \n",
    "    for i in range (att_k):\n",
    "        for j in range(8):\n",
    "            weights_1[i][j] = skimage.util.random_noise(weights_1[i][j],mode=\"gaussian\",var = 0.2,clip=True)\n",
    "        model_d[i].set_weights(weights_1[i])              \n",
    "    return model_d, weights_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d18f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7302997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#服务器将客户端数据集中并分配给客户端\n",
    "def fed_avg(model_d,indexes_below_mean):\n",
    "    weights = {}\n",
    "    loss_all=[]\n",
    "    acc_all=[]\n",
    " \n",
    "    #这个for是得到所有模型的参数，那在这里保存所有模型的参数\n",
    "    for i in range(len(model_d)): \n",
    "        weights[i] = model_d[i].get_weights() \n",
    "    index = indexes_below_mean[0] \n",
    "    last = index[len(index)-1]\n",
    " #################################################\n",
    "    weights_avg = []\n",
    "    weights_sum = weights[last]   #这也不能写保存最后一个了\n",
    "##################################################    \n",
    "    for i in range (len(weights[0])):  #  1个客户端的n层参数\n",
    "        for j in range (len (index)-1): #相当于n个客户端\n",
    "            j1= index[j]\n",
    "            weights_sum[i] = weights_sum[i]+ weights[j1][i]   #这里出现过错误，应该是先客户端【j】再客户端的层【i】\n",
    "        weights_sum[i] = weights_sum[i] /len(index)\n",
    "        weights_avg.append(weights_sum[i])\n",
    "    #将集合后的数据分配给每一个客户端\n",
    "    for i in range (len (weights)):\n",
    "        model_d[i].set_weights(weights_avg)\n",
    "    #测试客户端的精度\n",
    "    loss,accuracy= model_d[i].evaluate(x_test,y_test)\n",
    "#################################################   \n",
    "    return model_d,loss,accuracy\n",
    " #################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0e2849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FL_F (weights):\n",
    "    result_50 = []\n",
    "    for i in range(30):\n",
    "        extracted_vectors_1 = e_x(weights[i])\n",
    "        result = np.concatenate([extracted_vectors_1[0], extracted_vectors_1[1], extracted_vectors_1[2], extracted_vectors_1[3], extracted_vectors_1[4],extracted_vectors_1[5], extracted_vectors_1[6], extracted_vectors_1[7]])               \n",
    "        result_50.append(result)\n",
    "    matrix = np.vstack(result_50)\n",
    "    recons_50 = autoencoder.predict(matrix)   \n",
    "    euclidean_distances = np.sqrt(np.sum((recons_50 - matrix) ** 2, axis=1))\n",
    "# 计算euclidean_distances的均值\n",
    "    mean_distance = np.mean(euclidean_distances)\n",
    "# 找到小于均值的所有索引\n",
    "    indexes_below_mean = np.where(euclidean_distances < mean_distance)\n",
    "    print(indexes_below_mean[0])\n",
    "    return indexes_below_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5648cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3d21ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f258b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "557f47fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for i in range(15):\n",
    "#    data_x[i],data_y[i] = add_partial_backdoor(data_x[i],data_y[i])\n",
    "\n",
    "    \n",
    "    \n",
    "def add_backdoor_to_all(images, trigger_size=3, backdoor_label=8):\n",
    "    backdoor_images = []\n",
    "    backdoor_labels_onehot = []\n",
    "\n",
    "    for image in images:\n",
    "        backdoor_image = np.copy(image)\n",
    "        backdoor_image[-trigger_size:, -trigger_size:] = 1.0\n",
    "        backdoor_images.append(backdoor_image)\n",
    "\n",
    "    return np.array(backdoor_images)\n",
    "x_test_door = add_backdoor_to_all(x_test)\n",
    "y_test_oneshot_8 = np.zeros_like(y_test)\n",
    "y_test_oneshot_8[:, 8] = 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e9b6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f71161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "609a81a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.3967 - accuracy: 0.1060\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.1976 - accuracy: 0.2704\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8682 - accuracy: 0.4160\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.7387 - accuracy: 0.4259\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.6867 - accuracy: 0.4560\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.6499 - accuracy: 0.4539\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.5967 - accuracy: 0.4751\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.5754 - accuracy: 0.4843\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.5406 - accuracy: 0.4846\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.5324 - accuracy: 0.4769\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.5066 - accuracy: 0.5026\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4655 - accuracy: 0.5125\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4691 - accuracy: 0.5106\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4153 - accuracy: 0.5276\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4217 - accuracy: 0.5245\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4107 - accuracy: 0.5367\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3652 - accuracy: 0.5548\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3687 - accuracy: 0.5586\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3593 - accuracy: 0.5648\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3118 - accuracy: 0.5714\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2938 - accuracy: 0.5822\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3089 - accuracy: 0.5894\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2450 - accuracy: 0.6104\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2410 - accuracy: 0.6123\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.2313 - accuracy: 0.6142\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1960 - accuracy: 0.6246\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1993 - accuracy: 0.6241\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1623 - accuracy: 0.6406\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1490 - accuracy: 0.6513\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1253 - accuracy: 0.6545\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1207 - accuracy: 0.6576\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0888 - accuracy: 0.6695\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0818 - accuracy: 0.6696\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0673 - accuracy: 0.6798\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0369 - accuracy: 0.6887\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.0248 - accuracy: 0.6883\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.0108 - accuracy: 0.6960\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.9812 - accuracy: 0.7031\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0126 - accuracy: 0.6920\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9829 - accuracy: 0.7049\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9586 - accuracy: 0.7045\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.9628 - accuracy: 0.7081\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.9577 - accuracy: 0.7103\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9525 - accuracy: 0.7171\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.9287 - accuracy: 0.7182\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9093 - accuracy: 0.7266\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.9384 - accuracy: 0.7212\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.9057 - accuracy: 0.7289\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9162 - accuracy: 0.7305\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8918 - accuracy: 0.7308\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9015 - accuracy: 0.7316\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8903 - accuracy: 0.7369\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8794 - accuracy: 0.7383\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8713 - accuracy: 0.7416\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8713 - accuracy: 0.7455\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8531 - accuracy: 0.7469\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8620 - accuracy: 0.7479\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8489 - accuracy: 0.7505\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8601 - accuracy: 0.7501\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8382 - accuracy: 0.7523\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8540 - accuracy: 0.7530\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8334 - accuracy: 0.7564\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8400 - accuracy: 0.7582\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8282 - accuracy: 0.7609\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.8188 - accuracy: 0.7643\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8255 - accuracy: 0.7634\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8207 - accuracy: 0.7624\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8140 - accuracy: 0.7637\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8204 - accuracy: 0.7666\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8148 - accuracy: 0.7675\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8204 - accuracy: 0.7703\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8199 - accuracy: 0.7715\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8137 - accuracy: 0.7693\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8146 - accuracy: 0.7752\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7951 - accuracy: 0.7758\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8170 - accuracy: 0.7731\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7892 - accuracy: 0.7765\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7977 - accuracy: 0.7754\n",
      "***********************************************************************\n",
      "[ 8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8999 - accuracy: 0.7591\n",
      "***********************************************************************\n",
      "[ 4  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8785 - accuracy: 0.7369\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7807 - accuracy: 0.7756\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7975 - accuracy: 0.7770\n",
      "***********************************************************************\n",
      "[ 6  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7948 - accuracy: 0.7647\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7802 - accuracy: 0.7799\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7718 - accuracy: 0.7812\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7753 - accuracy: 0.7809\n",
      "***********************************************************************\n",
      "[ 1  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.7929 - accuracy: 0.7856\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7843 - accuracy: 0.7837\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.7797 - accuracy: 0.7830\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.7892 - accuracy: 0.7825\n",
      "***********************************************************************\n",
      "[ 2  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8432 - accuracy: 0.7707\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7803 - accuracy: 0.7823\n",
      "***********************************************************************\n",
      "[ 4  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8399 - accuracy: 0.7814\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.7949 - accuracy: 0.7812\n",
      "***********************************************************************\n",
      "[ 5  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9328 - accuracy: 0.7592\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7764 - accuracy: 0.7818\n",
      "***********************************************************************\n",
      "[ 1  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8509 - accuracy: 0.7631\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7816 - accuracy: 0.7836\n",
      "***********************************************************************\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7689 - accuracy: 0.7862\n"
     ]
    }
   ],
   "source": [
    "n=30\n",
    "model_d = []\n",
    "for i in range(30):\n",
    "    model = set_model()\n",
    "    model_d.append(model)\n",
    "    \n",
    "att_k = 9;\n",
    "\n",
    "for s in range (100):\n",
    "    model_d,weights_1 = fit_allmodel(model_d,att_k)\n",
    "    \n",
    "    print(\"***********************************************************************\")\n",
    "    indexes_below_mean = FL_F(weights_1)\n",
    "    model_d,loss_x_y,accuracy_x_y  = fed_avg(model_d,indexes_below_mean)\n",
    "# data_finall 太大，不能保存， 只能分开保存，想一下 如何一次运行能保存N个文件，试一下 程序名 +n 来进行保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fa98be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####需要再FL_F之前保存攻击的weight fed——all中 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aff68e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3970 - accuracy: 0.0583\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.1892 - accuracy: 0.2801\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.9234 - accuracy: 0.4009\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.7766 - accuracy: 0.4632\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.7169 - accuracy: 0.4453\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.6654 - accuracy: 0.4666\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.6218 - accuracy: 0.4742\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.5912 - accuracy: 0.4804\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.5658 - accuracy: 0.4717\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5255 - accuracy: 0.4972\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.5001 - accuracy: 0.4956\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4935 - accuracy: 0.4864\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4738 - accuracy: 0.4915\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4386 - accuracy: 0.5121\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4244 - accuracy: 0.5152\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4258 - accuracy: 0.5130\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4127 - accuracy: 0.5186\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4095 - accuracy: 0.5115\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3722 - accuracy: 0.5339\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "545/556 [============================>.] - ETA: 0s - loss: 1.3615 - accuracy: 0.5386***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3355 - accuracy: 0.5443\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3094 - accuracy: 0.5598\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3112 - accuracy: 0.5584\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3095 - accuracy: 0.5654\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2746 - accuracy: 0.5790\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2706 - accuracy: 0.5830\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2680 - accuracy: 0.5835\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2420 - accuracy: 0.5890\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2057 - accuracy: 0.6093\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1879 - accuracy: 0.6168\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1768 - accuracy: 0.6221\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1430 - accuracy: 0.6359\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.1417 - accuracy: 0.6378\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.1228 - accuracy: 0.6418\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1198 - accuracy: 0.6435\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0825 - accuracy: 0.6581\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0626 - accuracy: 0.6654\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0466 - accuracy: 0.6698\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0271 - accuracy: 0.6756\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0190 - accuracy: 0.6802\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0095 - accuracy: 0.6838\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0086 - accuracy: 0.6831\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9921 - accuracy: 0.6905\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9710 - accuracy: 0.6927\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9436 - accuracy: 0.7028\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.9444 - accuracy: 0.7014\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9298 - accuracy: 0.7042\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9267 - accuracy: 0.7091\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8971 - accuracy: 0.7176\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9079 - accuracy: 0.7144\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8840 - accuracy: 0.7233\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8746 - accuracy: 0.7270\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8783 - accuracy: 0.7262\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8642 - accuracy: 0.7294\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8731 - accuracy: 0.7302\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8513 - accuracy: 0.7357\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8444 - accuracy: 0.7387\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8383 - accuracy: 0.7390\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8264 - accuracy: 0.7436\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8265 - accuracy: 0.7464\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8233 - accuracy: 0.7436\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8202 - accuracy: 0.7441\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8044 - accuracy: 0.7518\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8081 - accuracy: 0.7530\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7937 - accuracy: 0.7568\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8018 - accuracy: 0.7549\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7938 - accuracy: 0.7544\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7883 - accuracy: 0.7587\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7933 - accuracy: 0.7563\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7791 - accuracy: 0.7603\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7843 - accuracy: 0.7627\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7726 - accuracy: 0.7614\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7772 - accuracy: 0.7652\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7969 - accuracy: 0.7611\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7708 - accuracy: 0.7662\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7741 - accuracy: 0.7656\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7678 - accuracy: 0.7679\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7608 - accuracy: 0.7694\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7522 - accuracy: 0.7717\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7727 - accuracy: 0.7697\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7589 - accuracy: 0.7719\n",
      "***********************************************************************\n",
      "[ 0  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7536 - accuracy: 0.7587\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7490 - accuracy: 0.7718\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7408 - accuracy: 0.7735\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7567 - accuracy: 0.7718\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7536 - accuracy: 0.7715\n",
      "***********************************************************************\n",
      "[ 2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7951 - accuracy: 0.7603\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7385 - accuracy: 0.7777\n",
      "***********************************************************************\n",
      "[ 2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7751 - accuracy: 0.7638\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7567 - accuracy: 0.7755\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7369 - accuracy: 0.7793\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7404 - accuracy: 0.7777\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7564 - accuracy: 0.7785\n",
      "***********************************************************************\n",
      "[ 2  3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.7840 - accuracy: 0.7754\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7519 - accuracy: 0.7743\n",
      "***********************************************************************\n",
      "[ 1  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8714 - accuracy: 0.7652\n",
      "***********************************************************************\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "287/556 [==============>...............] - ETA: 0s - loss: 0.7350 - accuracy: 0.7786"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n=30\n",
    "model_d = []\n",
    "for i in range(30):\n",
    "    model = set_model()\n",
    "    model_d.append(model)\n",
    "    \n",
    "att_k = 6;\n",
    "\n",
    "for s in range (100):\n",
    "    model_d,weights_1 = fit_allmodel(model_d,att_k)\n",
    "    \n",
    "    print(\"***********************************************************************\")\n",
    "    indexes_below_mean = FL_F(weights_1)\n",
    "    model_d,loss_x_y,accuracy_x_y  = fed_avg(model_d,indexes_below_mean)\n",
    "# data_finall 太大，不能保存， 只能分开保存，想一下 如何一次运行能保存N个文件，试一下 程序名 +n 来进行保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2b481a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9cd3d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4002 - accuracy: 0.1273\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.1572 - accuracy: 0.3289\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8613 - accuracy: 0.4412\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.6986 - accuracy: 0.4571\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.6402 - accuracy: 0.4708\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.6080 - accuracy: 0.4673\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.5575 - accuracy: 0.4867\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.5321 - accuracy: 0.4827\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.5058 - accuracy: 0.4899\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4669 - accuracy: 0.5046\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4363 - accuracy: 0.5129\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4307 - accuracy: 0.5074\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3964 - accuracy: 0.5209\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3766 - accuracy: 0.5282\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3796 - accuracy: 0.5259\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3403 - accuracy: 0.5410\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3535 - accuracy: 0.5353\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3329 - accuracy: 0.5422\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3344 - accuracy: 0.5418\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3121 - accuracy: 0.5502\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3255 - accuracy: 0.5437\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3034 - accuracy: 0.5507\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3002 - accuracy: 0.5544\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2714 - accuracy: 0.5650\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2921 - accuracy: 0.5578\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2570 - accuracy: 0.5694\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2579 - accuracy: 0.5713\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2198 - accuracy: 0.5861\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2026 - accuracy: 0.5947\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1900 - accuracy: 0.6028\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1634 - accuracy: 0.6154\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1457 - accuracy: 0.6195\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1304 - accuracy: 0.6262\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1176 - accuracy: 0.6299\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0785 - accuracy: 0.6403\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0809 - accuracy: 0.6427\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0672 - accuracy: 0.6459\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0614 - accuracy: 0.6477\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0476 - accuracy: 0.6509\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0370 - accuracy: 0.6559\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0273 - accuracy: 0.6632\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9960 - accuracy: 0.6697\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0075 - accuracy: 0.6673\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0057 - accuracy: 0.6668\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9828 - accuracy: 0.6762\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9720 - accuracy: 0.6774\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9527 - accuracy: 0.6857\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9505 - accuracy: 0.6880\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9290 - accuracy: 0.6938\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9223 - accuracy: 0.6982\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9198 - accuracy: 0.6989\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9023 - accuracy: 0.7040\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8938 - accuracy: 0.7077\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8885 - accuracy: 0.7074\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8820 - accuracy: 0.7122\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8712 - accuracy: 0.7170\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8620 - accuracy: 0.7216\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8666 - accuracy: 0.7174\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8502 - accuracy: 0.7253\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8465 - accuracy: 0.7289\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8399 - accuracy: 0.7298\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8493 - accuracy: 0.7282\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8259 - accuracy: 0.7366\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8206 - accuracy: 0.7357\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8198 - accuracy: 0.7392\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8104 - accuracy: 0.7416\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8129 - accuracy: 0.7423\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7960 - accuracy: 0.7453\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.8018 - accuracy: 0.7459\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7845 - accuracy: 0.7524\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7946 - accuracy: 0.7477\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7886 - accuracy: 0.7510\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7829 - accuracy: 0.7549\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7805 - accuracy: 0.7530\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7782 - accuracy: 0.7550\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7767 - accuracy: 0.7563\n",
      "***********************************************************************\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "371/556 [===================>..........] - ETA: 0s - loss: 0.7602 - accuracy: 0.7616"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n=30\n",
    "model_d = []\n",
    "for i in range(30):\n",
    "    model = set_model()\n",
    "    model_d.append(model)\n",
    "    \n",
    "att_k = 3;\n",
    "\n",
    "for s in range (100):\n",
    "    model_d,weights_1 = fit_allmodel(model_d,att_k)\n",
    "    \n",
    "    print(\"***********************************************************************\")\n",
    "    indexes_below_mean = FL_F(weights_1)\n",
    "    model_d,loss_x_y,accuracy_x_y  = fed_avg(model_d,indexes_below_mean)\n",
    "# data_finall 太大，不能保存， 只能分开保存，想一下 如何一次运行能保存N个文件，试一下 程序名 +n 来进行保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81605e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bd9c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5734d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33628706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
