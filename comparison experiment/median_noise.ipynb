{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 23:09:52.486560: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Convolution2D,MaxPooling2D,Flatten\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Convolution2D,MaxPooling2D,Flatten\n",
    "from keras.optimizers import Adam\n",
    "import spams\n",
    "import numpy as np\n",
    "import skimage \n",
    "import tensorflow as tf\n",
    "import math as m\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras import layers\n",
    "from mxnet import nd\n",
    "import mxnet as mx\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import losses\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0:  (array([3, 4, 6, 7, 8], dtype=uint8), array([ 97., 238., 353., 236., 228.], dtype=float32))\n",
      " 1:  (array([ 2,  3,  4,  7, 10], dtype=uint8), array([111., 105., 315., 325., 296.], dtype=float32))\n",
      " 2:  (array([2, 3, 4, 7, 8], dtype=uint8), array([103., 106., 298., 337., 308.], dtype=float32))\n",
      " 3:  (array([1, 2, 3, 5, 6], dtype=uint8), array([111., 116., 116., 309., 500.], dtype=float32))\n",
      " 4:  (array([1, 2, 3, 5, 9], dtype=uint8), array([123., 155., 160., 418., 296.], dtype=float32))\n",
      " 5:  (array([ 1,  3,  7,  8, 10], dtype=uint8), array([106., 120., 330., 313., 283.], dtype=float32))\n",
      " 6:  (array([ 0,  6,  7,  8, 10], dtype=uint8), array([113., 384., 239., 220., 196.], dtype=float32))\n",
      " 7:  (array([0, 2, 6, 7, 9], dtype=uint8), array([141., 118., 402., 264., 227.], dtype=float32))\n",
      " 8:  (array([0, 1, 2, 4, 9], dtype=uint8), array([185., 130., 138., 375., 324.], dtype=float32))\n",
      " 9:  (array([ 1,  4,  7,  8, 10], dtype=uint8), array([ 97., 274., 256., 280., 245.], dtype=float32))\n",
      " 10:  (array([ 1,  3,  6,  7, 10], dtype=uint8), array([ 92.,  96., 429., 268., 267.], dtype=float32))\n",
      " 11:  (array([0, 4, 7, 8, 9], dtype=uint8), array([137., 274., 270., 278., 193.], dtype=float32))\n",
      " 12:  (array([2, 3, 4, 5, 9], dtype=uint8), array([113., 114., 316., 337., 272.], dtype=float32))\n",
      " 13:  (array([ 0,  1,  7,  9, 10], dtype=uint8), array([152., 116., 336., 253., 295.], dtype=float32))\n",
      " 14:  (array([0, 2, 3, 4, 8], dtype=uint8), array([175., 134., 121., 363., 359.], dtype=float32))\n",
      " 15:  (array([2, 3, 4, 8, 9], dtype=uint8), array([111., 115., 323., 350., 253.], dtype=float32))\n",
      " 16:  (array([1, 3, 4, 5, 9], dtype=uint8), array([104., 135., 342., 312., 259.], dtype=float32))\n",
      " 17:  (array([0, 1, 3, 4, 6], dtype=uint8), array([134., 114., 116., 312., 476.], dtype=float32))\n",
      " 18:  (array([ 0,  7,  8,  9, 10], dtype=uint8), array([140., 307., 268., 201., 236.], dtype=float32))\n",
      " 19:  (array([0, 3, 5, 6, 9], dtype=uint8), array([142.,  93., 279., 416., 222.], dtype=float32))\n",
      " 20:  (array([0, 3, 6, 7, 8], dtype=uint8), array([135., 117., 408., 243., 249.], dtype=float32))\n",
      " 21:  (array([1, 2, 3, 6, 9], dtype=uint8), array([137., 116., 131., 517., 251.], dtype=float32))\n",
      " 22:  (array([3, 4, 5, 6, 8], dtype=uint8), array([ 81., 232., 223., 366., 250.], dtype=float32))\n",
      " 23:  (array([ 3,  6,  7,  8, 10], dtype=uint8), array([ 92., 373., 226., 228., 233.], dtype=float32))\n",
      " 24:  (array([ 1,  4,  5,  6, 10], dtype=uint8), array([ 86., 269., 215., 397., 185.], dtype=float32))\n",
      " 25:  (array([ 1,  2,  3,  5, 10], dtype=uint8), array([133., 141., 150., 370., 358.], dtype=float32))\n",
      " 26:  (array([0, 1, 3, 4, 5], dtype=uint8), array([172., 133., 148., 365., 334.], dtype=float32))\n",
      " 27:  (array([ 3,  6,  8,  9, 10], dtype=uint8), array([108., 387., 236., 206., 215.], dtype=float32))\n",
      " 28:  (array([ 1,  3,  4,  5, 10], dtype=uint8), array([115., 121., 338., 307., 271.], dtype=float32))\n",
      " 29:  (array([ 0,  1,  5,  8, 10], dtype=uint8), array([141., 112., 319., 287., 293.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "# 加载数据''\n",
    "with open('/home/xipeng/FLcode/Organa/my_data_0.5.pkl', 'rb') as f:\n",
    "    data_x , data_y,x_test,y_test  = pickle.load(f)\n",
    "x_train =  data_x \n",
    "y_train =  data_y\n",
    "num_classes = 11\n",
    "for i in range(30):\n",
    "    df = pd.DataFrame(data_y[i])\n",
    "    label_counts = df.sum(axis=0)\n",
    "    label_counts = label_counts[label_counts > 0]\n",
    "# 提取标签和数量\n",
    "    labels = np.array(label_counts.index, dtype=np.uint8)\n",
    "    counts = np.array(label_counts.values)\n",
    "# 打印结果\n",
    "    print(f\" {i}: \",(labels, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对前百分之10的数据进行噪声处理\n",
    "#for i in range(6000):\n",
    "    \n",
    "   # x_train[i,:,:] = skimage.util.random_noise(x_train[i,:,:],mode=\"gaussian\",var=0.1,clip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_data(x_train,y_train,n):   \n",
    "    data_x = {}\n",
    "    data_y = {}\n",
    "    t = int(60000/n)\n",
    "    print(t)\n",
    "    print(n)\n",
    "    for i in range(n):\n",
    "        data_x[i] = x_train[t*i:t+t*i,:,:]\n",
    "\n",
    "        data_y[i] = y_train[t*i:t+t*i,:]\n",
    "\n",
    "    print(data_x[n-1].shape)\n",
    "    print(data_y[n-1].shape)\n",
    "    return data_x,data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  set_model(n):\n",
    "    model_d = {}\n",
    "    for i in range(n):\n",
    "        model_d[i]=Sequential()\n",
    "        # 定义顺序模型\n",
    "        model = model_d[i]\n",
    "\n",
    "        model.add(Convolution2D(\n",
    "            input_shape = (28,28,1),\n",
    "            filters = 8,\n",
    "            kernel_size = 3,\n",
    "            strides = 1,\n",
    "            padding = 'same',\n",
    "            activation = 'relu',\n",
    "        ))\n",
    "    # 第一个池化层\n",
    "        model.add(MaxPooling2D(\n",
    "            pool_size = 2,\n",
    "            strides = 2,\n",
    "            padding = 'same',\n",
    "        ))\n",
    "    # 第二个卷积层\n",
    "        model.add(Convolution2D(16,3,strides=1,padding='same',activation='relu'))\n",
    "    # 第二个池化层\n",
    "        model.add(MaxPooling2D(2,2,'same'))\n",
    "    # 把第二个池化层的输出扁平化为1维\n",
    "        model.add(Flatten())\n",
    "    # 第一个全连接层\n",
    "        model.add(Dense(256,activation='relu'))\n",
    "    # Dropout\n",
    "        model.add(Dropout(0.5))\n",
    "    # 第二个全连接层\n",
    "        model.add(Dense(11,activation='softmax'))\n",
    "\n",
    "    # 定义优化器\n",
    "        adam = Adam(lr=5e-4)\n",
    "\n",
    "    # 定义优化器,loss function,训练过程中计算准确率\n",
    "        model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "        #print(model_d[i])\n",
    "    return model_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def 定义函数 、\n",
    "#return 返回函数\n",
    "#想要使用函数中定义的值，想要 变量名= 函数值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#所有的客户端训练一遍\n",
    "def fit_allmodel(model_d):\n",
    "    weights_1 = {}\n",
    "    for i in range(len(model_d)):\n",
    "    # 训练模型\n",
    "        model_d[i].fit(data_x[i],data_y[i],batch_size=64,epochs=1,verbose=0)\n",
    "        loss,accuracy = model_d[i].evaluate(x_test,y_test,verbose=0)\n",
    "        \n",
    "        #print('test loss',loss)\n",
    "        #print('test accuracy',accuracy)\n",
    "        \n",
    "    for i in range(len(model_d)): \n",
    "        weights_1[i] = model_d[i].get_weights() \n",
    "    return model_d , weights_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_median_params(models):\n",
    "    \"\"\"\n",
    "    计算一系列模型参数的中位数。\n",
    "\n",
    "    参数:\n",
    "        models: 一系列训练好的模型。\n",
    "\n",
    "    返回值:\n",
    "        中位数参数列表。\n",
    "    \"\"\"\n",
    "    params = [model.get_weights() for model in models]\n",
    "    median_params = [np.median([param[i] for param in params], axis=0) for i in range(len(params[0]))]\n",
    "    return median_params\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#服务器将客户端数据集中并分配给客户端\n",
    "def fed_avg(model_d):\n",
    "    weights = {}\n",
    "    loss_all=[]\n",
    "    acc_all=[]\n",
    "    data_save={}\n",
    "    #这个for是得到所有模型的参数，那在这里保存所有模型的参数\n",
    "    for i in range(len(model_d)): \n",
    "        weights[i] = model_d[i].get_weights() \n",
    "#################################################        \n",
    "#对前5个数据进行        \n",
    "    for i in range(10):\n",
    "        for j in range(8):\n",
    "            weights[i][j] = skimage.util.random_noise(weights[i][j],mode=\"gaussian\",var = 0.2,clip=True)\n",
    "        \n",
    "        \n",
    "#################################################\n",
    "#保存50个变量 \n",
    "\n",
    "####这里把形状print出来，然后修改形状。\n",
    "    M_weight = mdeian_weight(weights)\n",
    "    M_weight[0] = np.reshape(M_weight[0], (5, 5, 1, 30))\n",
    "    M_weight[1] = np.reshape(M_weight[1], (30,))\n",
    "    M_weight[2] = np.reshape(M_weight[2], (5, 5, 30, 5))\n",
    "    M_weight[3] = np.reshape(M_weight[3], (5,))\n",
    "    M_weight[4] = np.reshape(M_weight[4], (245, 100))\n",
    "    M_weight[5] = np.reshape(M_weight[5],(100,))\n",
    "    M_weight[6] = np.reshape(M_weight[6], (100, 10))\n",
    "    M_weight[7] = np.reshape(M_weight[7], (10, ))\n",
    "    #将集合后的数据分配给每一个客户端\n",
    "    for i in range (50):\n",
    "        model_d[i].set_weights(M_weight)\n",
    "    \n",
    "    #测试客户端的精度\n",
    "    loss,accuracy= model_d[i].evaluate(x_test,y_test)\n",
    "#################################################   \n",
    "    return model_d,loss,accuracy\n",
    " #################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdeian_weight(weights):\n",
    "    weight_median = weights[0].copy()\n",
    "    \n",
    "    for t in range (8):  \n",
    "        tensors = []\n",
    "        for i in range(len(weights)):\n",
    "            tensors.append(weights[i][t])\n",
    "        stacked_tensors = np.stack(tensors)\n",
    "# Alternatively, you can use a list comprehension\n",
    "# tensors = [tensor[i] for i in range(num_tensors)]\n",
    "        weight_median[t] = np.median(stacked_tensors, axis=0)\n",
    "# Stack the tensors along a new axis (axis 0 by default)\n",
    "    return weight_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(weights,t):\n",
    "    for i in range(t):\n",
    "        for j in range(8):\n",
    "            weights[i][j] = skimage.util.random_noise(weights[i][j],mode=\"gaussian\",var = 0.2,clip=True)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_set(model_d,avg_weight):\n",
    "    #这个for是得到所有模型的参数，那在这里保存所有模型的参数\n",
    "    for i in range(len(model_d)): \n",
    "        model_d[i].set_weights(avg_weight) \n",
    "    loss,accuracy= model_d[0].evaluate(x_test,y_test)\n",
    "    return model_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.models import clone_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-17 23:09:58.483921: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2023-06-17 23:09:58.519725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:af:00.0 name: NVIDIA Tesla V100-PCIE-16GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-06-17 23:09:58.519788: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-06-17 23:09:58.519862: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2023-06-17 23:09:58.519897: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2023-06-17 23:09:58.519931: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2023-06-17 23:09:58.519963: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-06-17 23:09:58.521643: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-06-17 23:09:58.521740: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-06-17 23:09:58.524513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2023-06-17 23:09:58.525584: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-17 23:09:58.539705: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 1700000000 Hz\n",
      "2023-06-17 23:09:58.541220: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563a8393a7c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-17 23:09:58.541273: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-06-17 23:09:58.871173: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563a839a6b90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-17 23:09:58.871256: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA Tesla V100-PCIE-16GB, Compute Capability 7.0\n",
      "2023-06-17 23:09:58.879767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:af:00.0 name: NVIDIA Tesla V100-PCIE-16GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-06-17 23:09:58.879869: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-06-17 23:09:58.879939: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2023-06-17 23:09:58.879983: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2023-06-17 23:09:58.880028: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2023-06-17 23:09:58.880071: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-06-17 23:09:58.880127: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-06-17 23:09:58.880171: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-06-17 23:09:58.885920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2023-06-17 23:09:58.886032: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-06-17 23:10:04.956126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-06-17 23:10:04.956203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
      "2023-06-17 23:10:04.956220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
      "2023-06-17 23:10:04.979808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14216 MB memory) -> physical GPU (device: 0, name: NVIDIA Tesla V100-PCIE-16GB, pci bus id: 0000:af:00.0, compute capability: 7.0)\n",
      "2023-06-17 23:10:08.871865: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2023-06-17 23:10:09.319756: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-06-17 23:10:11.617505: W tensorflow/stream_executor/gpu/asm_compiler.cc:81] Running ptxas --version returned 256\n",
      "2023-06-17 23:10:11.853534: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3973 - accuracy: 0.1138\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3463 - accuracy: 0.1161\n",
      "1\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.3878 - accuracy: 0.1226\n",
      "2\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.2546 - accuracy: 0.3107\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.1969 - accuracy: 0.2083\n",
      "4\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.0837 - accuracy: 0.2622\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.0325 - accuracy: 0.2909\n",
      "6\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.9559 - accuracy: 0.3169\n",
      "7\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.9162 - accuracy: 0.3462\n",
      "8\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.8574 - accuracy: 0.3586\n",
      "9\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.8054 - accuracy: 0.4006\n",
      "10\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.7978 - accuracy: 0.3799\n",
      "11\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.7392 - accuracy: 0.4412\n",
      "12\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.7446 - accuracy: 0.4332\n",
      "13\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.7089 - accuracy: 0.4363\n",
      "14\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.6872 - accuracy: 0.4602\n",
      "15\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.6683 - accuracy: 0.4612\n",
      "16\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.6333 - accuracy: 0.4906\n",
      "17\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.6418 - accuracy: 0.4672\n",
      "18\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.6084 - accuracy: 0.5092\n",
      "19\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.6278 - accuracy: 0.4598\n",
      "20\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5856 - accuracy: 0.5100\n",
      "21\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.6412 - accuracy: 0.4642\n",
      "22\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5622 - accuracy: 0.5123\n",
      "23\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5951 - accuracy: 0.4873\n",
      "24\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5664 - accuracy: 0.5011\n",
      "25\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5600 - accuracy: 0.4983\n",
      "26\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5435 - accuracy: 0.5058\n",
      "27\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5307 - accuracy: 0.5203\n",
      "28\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5429 - accuracy: 0.5120\n",
      "29\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5278 - accuracy: 0.5137\n",
      "30\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5415 - accuracy: 0.5024\n",
      "31\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5459 - accuracy: 0.5245\n",
      "32\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5288 - accuracy: 0.5165\n",
      "33\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5126 - accuracy: 0.5198\n",
      "34\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5360 - accuracy: 0.5000\n",
      "35\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5113 - accuracy: 0.5223\n",
      "36\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4982 - accuracy: 0.5196\n",
      "37\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5183 - accuracy: 0.5302\n",
      "38\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4809 - accuracy: 0.5379\n",
      "39\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5052 - accuracy: 0.5281\n",
      "40\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4839 - accuracy: 0.5455\n",
      "41\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4911 - accuracy: 0.5294\n",
      "42\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4720 - accuracy: 0.5441\n",
      "43\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4813 - accuracy: 0.5456\n",
      "44\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4703 - accuracy: 0.5462\n",
      "45\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4675 - accuracy: 0.5571\n",
      "46\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4868 - accuracy: 0.5369\n",
      "47\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4676 - accuracy: 0.5541\n",
      "48\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4738 - accuracy: 0.5403\n",
      "49\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4540 - accuracy: 0.5560\n",
      "50\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4744 - accuracy: 0.5453\n",
      "51\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4542 - accuracy: 0.5470\n",
      "52\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4381 - accuracy: 0.5609\n",
      "53\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4435 - accuracy: 0.5594\n",
      "54\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4439 - accuracy: 0.5507\n",
      "55\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4118 - accuracy: 0.5670\n",
      "56\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4408 - accuracy: 0.5658\n",
      "57\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4264 - accuracy: 0.5636\n",
      "58\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4325 - accuracy: 0.5652\n",
      "59\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4169 - accuracy: 0.5727\n",
      "60\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4251 - accuracy: 0.5721\n",
      "61\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3917 - accuracy: 0.5842\n",
      "62\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3845 - accuracy: 0.5806\n",
      "63\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4028 - accuracy: 0.5760\n",
      "64\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3997 - accuracy: 0.5778\n",
      "65\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4065 - accuracy: 0.5842\n",
      "66\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3842 - accuracy: 0.5798\n",
      "67\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3898 - accuracy: 0.5815\n",
      "68\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3922 - accuracy: 0.5883\n",
      "69\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3706 - accuracy: 0.5950\n",
      "70\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3783 - accuracy: 0.5876\n",
      "71\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3626 - accuracy: 0.5876\n",
      "72\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3579 - accuracy: 0.5891\n",
      "73\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3733 - accuracy: 0.5832\n",
      "74\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3705 - accuracy: 0.5889\n",
      "75\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3232 - accuracy: 0.6032\n",
      "76\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3504 - accuracy: 0.5904\n",
      "77\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3295 - accuracy: 0.5958\n",
      "78\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3313 - accuracy: 0.5937\n",
      "79\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3322 - accuracy: 0.5971\n",
      "80\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3358 - accuracy: 0.5943\n",
      "81\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3155 - accuracy: 0.6056\n",
      "82\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3196 - accuracy: 0.6032\n",
      "83\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.2955 - accuracy: 0.6051\n",
      "84\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3145 - accuracy: 0.6016\n",
      "85\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.2996 - accuracy: 0.6045\n",
      "86\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.2880 - accuracy: 0.6102\n",
      "87\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.2907 - accuracy: 0.6053\n",
      "88\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.2990 - accuracy: 0.6105\n",
      "89\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.2815 - accuracy: 0.6073\n",
      "90\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2768 - accuracy: 0.6071\n",
      "91\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.2621 - accuracy: 0.6157\n",
      "92\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2914 - accuracy: 0.6044\n",
      "93\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2635 - accuracy: 0.6156\n",
      "94\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.2712 - accuracy: 0.6123\n",
      "95\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.2643 - accuracy: 0.6141\n",
      "96\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.2575 - accuracy: 0.6118\n",
      "97\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2393 - accuracy: 0.6189\n",
      "98\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2433 - accuracy: 0.6207\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n = int(30)\n",
    "model_d = set_model(n)\n",
    "loss_fed = [0]*100\n",
    "accuracy_fed = [0]*100\n",
    "\n",
    "t= 9;\n",
    "for s in range (100):\n",
    "    model_d,weights_1 = fit_allmodel(model_d)\n",
    "    weights_1 = noise(weights_1,t)\n",
    "    avg_weight =   mdeian_weight(weights_1)\n",
    "    model_d = avg_set(model_d,avg_weight)\n",
    "    model_d[0].save('1N9.h5')\n",
    "    print(s)\n",
    "# data_finall 太大，不能保存， 只能分开保存，想一下 如何一次运行能保存N个文件，试一下 程序名 +n 来进行保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3978 - accuracy: 0.1182\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3678 - accuracy: 0.1794\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3824 - accuracy: 0.1161\n",
      "2\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3831 - accuracy: 0.1161\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.2820 - accuracy: 0.2094\n",
      "4\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.2203 - accuracy: 0.2401\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.1548 - accuracy: 0.2555\n",
      "6\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.0420 - accuracy: 0.2917\n",
      "7\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.9652 - accuracy: 0.3368\n",
      "8\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.9020 - accuracy: 0.3703\n",
      "9\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.8653 - accuracy: 0.3720\n",
      "10\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.8119 - accuracy: 0.4312\n",
      "11\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.8025 - accuracy: 0.4313\n",
      "12\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.7800 - accuracy: 0.4259\n",
      "13\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.7559 - accuracy: 0.4425\n",
      "14\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.7228 - accuracy: 0.4570\n",
      "15\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.7117 - accuracy: 0.4520\n",
      "16\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.6950 - accuracy: 0.4479\n",
      "17\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.6668 - accuracy: 0.4868\n",
      "18\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.6766 - accuracy: 0.4685\n",
      "19\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.6525 - accuracy: 0.4735\n",
      "20\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.6613 - accuracy: 0.4703\n",
      "21\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.6281 - accuracy: 0.4754\n",
      "22\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.6636 - accuracy: 0.4726\n",
      "23\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.6364 - accuracy: 0.4709\n",
      "24\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.6155 - accuracy: 0.4911\n",
      "25\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.6146 - accuracy: 0.4795\n",
      "26\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5998 - accuracy: 0.5036\n",
      "27\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5812 - accuracy: 0.4956\n",
      "28\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5848 - accuracy: 0.5146\n",
      "29\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5668 - accuracy: 0.4991\n",
      "30\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.5646 - accuracy: 0.5077\n",
      "31\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5779 - accuracy: 0.4987\n",
      "32\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5360 - accuracy: 0.5179\n",
      "33\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5566 - accuracy: 0.5105\n",
      "34\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5493 - accuracy: 0.5170\n",
      "35\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5601 - accuracy: 0.4981\n",
      "36\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5179 - accuracy: 0.5217\n",
      "37\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.5395 - accuracy: 0.5065\n",
      "38\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5260 - accuracy: 0.5116\n",
      "39\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5255 - accuracy: 0.5134\n",
      "40\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.5160 - accuracy: 0.5083\n",
      "41\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5370 - accuracy: 0.5002\n",
      "42\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5249 - accuracy: 0.5096\n",
      "43\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5099 - accuracy: 0.5238\n",
      "44\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5120 - accuracy: 0.5095\n",
      "45\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5178 - accuracy: 0.5133\n",
      "46\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5105 - accuracy: 0.5316\n",
      "47\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5121 - accuracy: 0.5161\n",
      "48\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5297 - accuracy: 0.5199\n",
      "49\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4936 - accuracy: 0.5232\n",
      "50\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5161 - accuracy: 0.5301\n",
      "51\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5095 - accuracy: 0.5278\n",
      "52\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5146 - accuracy: 0.5377\n",
      "53\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4982 - accuracy: 0.5265\n",
      "54\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4961 - accuracy: 0.5391\n",
      "55\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5266 - accuracy: 0.5217\n",
      "56\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5151 - accuracy: 0.5388\n",
      "57\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4971 - accuracy: 0.5301\n",
      "58\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5147 - accuracy: 0.5224\n",
      "59\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5027 - accuracy: 0.5288\n",
      "60\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4835 - accuracy: 0.5321\n",
      "61\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4774 - accuracy: 0.5392\n",
      "62\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4940 - accuracy: 0.5282\n",
      "63\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4935 - accuracy: 0.5313\n",
      "64\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4742 - accuracy: 0.5403\n",
      "65\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4636 - accuracy: 0.5387\n",
      "66\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4614 - accuracy: 0.5426\n",
      "67\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4345 - accuracy: 0.5532\n",
      "68\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4595 - accuracy: 0.5382\n",
      "69\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4377 - accuracy: 0.5383\n",
      "70\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4510 - accuracy: 0.5429\n",
      "71\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4446 - accuracy: 0.5602\n",
      "72\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4543 - accuracy: 0.5425\n",
      "73\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4566 - accuracy: 0.5440\n",
      "74\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4666 - accuracy: 0.5403\n",
      "75\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4520 - accuracy: 0.5436\n",
      "76\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4727 - accuracy: 0.5427\n",
      "77\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4600 - accuracy: 0.5382\n",
      "78\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4303 - accuracy: 0.5500\n",
      "79\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4371 - accuracy: 0.5338\n",
      "80\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4302 - accuracy: 0.5399\n",
      "81\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4131 - accuracy: 0.5528\n",
      "82\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4152 - accuracy: 0.5418\n",
      "83\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3994 - accuracy: 0.5626\n",
      "84\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3978 - accuracy: 0.5606\n",
      "85\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4143 - accuracy: 0.5512\n",
      "86\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4123 - accuracy: 0.5591\n",
      "87\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4203 - accuracy: 0.5511\n",
      "88\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3832 - accuracy: 0.5660\n",
      "89\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4157 - accuracy: 0.5462\n",
      "90\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4186 - accuracy: 0.5577\n",
      "91\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3914 - accuracy: 0.5632\n",
      "92\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3869 - accuracy: 0.5654\n",
      "93\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3806 - accuracy: 0.5728\n",
      "94\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3879 - accuracy: 0.5718\n",
      "95\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3738 - accuracy: 0.5602\n",
      "96\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3810 - accuracy: 0.5691\n",
      "97\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3790 - accuracy: 0.5736\n",
      "98\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3773 - accuracy: 0.5734\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n = int(30)\n",
    "model_d = set_model(n)\n",
    "loss_fed = [0]*100\n",
    "accuracy_fed = [0]*100\n",
    "\n",
    "t=6;\n",
    "for s in range (100):\n",
    "    model_d,weights_1 = fit_allmodel(model_d)\n",
    "    weights_1 = noise(weights_1,t)\n",
    "    avg_weight =   mdeian_weight(weights_1)\n",
    "    model_d = avg_set(model_d,avg_weight)\n",
    "    print(s)\n",
    "    model_d[0].save('1N6.h5')\n",
    "# data_finall 太大，不能保存， 只能分开保存，想一下 如何一次运行能保存N个文件，试一下 程序名 +n 来进行保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3986 - accuracy: 0.0429\n",
      "0\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3692 - accuracy: 0.1848\n",
      "1\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3710 - accuracy: 0.1848\n",
      "2\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3620 - accuracy: 0.1880\n",
      "3\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.2560 - accuracy: 0.2910\n",
      "4\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.1939 - accuracy: 0.1812\n",
      "5\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.1090 - accuracy: 0.2502\n",
      "6\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.0044 - accuracy: 0.3021\n",
      "7\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.9259 - accuracy: 0.3465\n",
      "8\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.8585 - accuracy: 0.3533\n",
      "9\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.8017 - accuracy: 0.3742\n",
      "10\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.7728 - accuracy: 0.3795\n",
      "11\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.7383 - accuracy: 0.3987\n",
      "12\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.7127 - accuracy: 0.4177\n",
      "13\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.6989 - accuracy: 0.4025\n",
      "14\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.6774 - accuracy: 0.4220\n",
      "15\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.6663 - accuracy: 0.4184\n",
      "16\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.6618 - accuracy: 0.4042\n",
      "17\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.6295 - accuracy: 0.4315\n",
      "18\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.6134 - accuracy: 0.4344\n",
      "19\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.6148 - accuracy: 0.4445\n",
      "20\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.5877 - accuracy: 0.4506\n",
      "21\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5980 - accuracy: 0.4548\n",
      "22\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5743 - accuracy: 0.4547\n",
      "23\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5546 - accuracy: 0.4765\n",
      "24\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5861 - accuracy: 0.4462\n",
      "25\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5264 - accuracy: 0.4889\n",
      "26\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5491 - accuracy: 0.4635\n",
      "27\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5255 - accuracy: 0.4820\n",
      "28\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5282 - accuracy: 0.4667\n",
      "29\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5099 - accuracy: 0.4908\n",
      "30\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4875 - accuracy: 0.4972\n",
      "31\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.5032 - accuracy: 0.4934\n",
      "32\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4999 - accuracy: 0.4880\n",
      "33\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4812 - accuracy: 0.4943\n",
      "34\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4929 - accuracy: 0.4903\n",
      "35\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4648 - accuracy: 0.4982\n",
      "36\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4676 - accuracy: 0.4934\n",
      "37\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4593 - accuracy: 0.5035\n",
      "38\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4361 - accuracy: 0.5073\n",
      "39\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4548 - accuracy: 0.5066\n",
      "40\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4295 - accuracy: 0.5044\n",
      "41\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4369 - accuracy: 0.5152\n",
      "42\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4339 - accuracy: 0.5078\n",
      "43\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4257 - accuracy: 0.5169\n",
      "44\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4555 - accuracy: 0.4918\n",
      "45\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4136 - accuracy: 0.5179\n",
      "46\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4093 - accuracy: 0.5219\n",
      "47\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4182 - accuracy: 0.5093\n",
      "48\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.4270 - accuracy: 0.5089\n",
      "49\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3937 - accuracy: 0.5179\n",
      "50\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3996 - accuracy: 0.5218\n",
      "51\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3923 - accuracy: 0.5146\n",
      "52\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3928 - accuracy: 0.5205\n",
      "53\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3746 - accuracy: 0.5240\n",
      "54\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3727 - accuracy: 0.5345\n",
      "55\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3745 - accuracy: 0.5234\n",
      "56\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3625 - accuracy: 0.5322\n",
      "57\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3812 - accuracy: 0.5244\n",
      "58\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3510 - accuracy: 0.5362\n",
      "59\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3546 - accuracy: 0.5359\n",
      "60\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3507 - accuracy: 0.5402\n",
      "61\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3523 - accuracy: 0.5350\n",
      "62\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3805 - accuracy: 0.5137\n",
      "63\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3316 - accuracy: 0.5474\n",
      "64\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3486 - accuracy: 0.5289\n",
      "65\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3228 - accuracy: 0.5460\n",
      "66\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3299 - accuracy: 0.5396\n",
      "67\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3203 - accuracy: 0.5473\n",
      "68\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3263 - accuracy: 0.5443\n",
      "69\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3149 - accuracy: 0.5437\n",
      "70\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3291 - accuracy: 0.5418\n",
      "71\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3168 - accuracy: 0.5447\n",
      "72\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3208 - accuracy: 0.5439\n",
      "73\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3132 - accuracy: 0.5475\n",
      "74\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.2989 - accuracy: 0.5541\n",
      "75\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3069 - accuracy: 0.5531\n",
      "76\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3056 - accuracy: 0.5461\n",
      "77\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.2930 - accuracy: 0.5599\n",
      "78\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.2851 - accuracy: 0.5533\n",
      "79\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.2979 - accuracy: 0.5527\n",
      "80\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.2740 - accuracy: 0.5541\n",
      "81\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.2708 - accuracy: 0.5655\n",
      "82\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.2791 - accuracy: 0.5588\n",
      "83\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2643 - accuracy: 0.5683\n",
      "84\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2750 - accuracy: 0.5585\n",
      "85\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2526 - accuracy: 0.5703\n",
      "86\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2569 - accuracy: 0.5689\n",
      "87\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2727 - accuracy: 0.5571\n",
      "88\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2582 - accuracy: 0.5663\n",
      "89\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2553 - accuracy: 0.5621\n",
      "90\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2460 - accuracy: 0.5734\n",
      "91\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2519 - accuracy: 0.5749\n",
      "92\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2429 - accuracy: 0.5704\n",
      "93\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2478 - accuracy: 0.5656\n",
      "94\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2509 - accuracy: 0.5660\n",
      "95\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2251 - accuracy: 0.5784\n",
      "96\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2346 - accuracy: 0.5768\n",
      "97\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2295 - accuracy: 0.5759\n",
      "98\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2306 - accuracy: 0.5813\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n = int(30)\n",
    "model_d = set_model(n)\n",
    "loss_fed = [0]*100\n",
    "accuracy_fed = [0]*100\n",
    "\n",
    "t=3;\n",
    "for s in range (100):\n",
    "    model_d,weights_1 = fit_allmodel(model_d)\n",
    "    weights_1 = noise(weights_1,t)\n",
    "    avg_weight =   mdeian_weight(weights_1)\n",
    "    model_d = avg_set(model_d,avg_weight)\n",
    "    print(s)\n",
    "    model_d[0].save('1N3.h5')\n",
    "# data_finall 太大，不能保存， 只能分开保存，想一下 如何一次运行能保存N个文件，试一下 程序名 +n 来进行保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
