{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2592b2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 00:31:24.194722: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Convolution2D,MaxPooling2D,Flatten\n",
    "from keras.optimizers import Adam\n",
    "import spams\n",
    "import numpy as np\n",
    "import skimage \n",
    "import tensorflow as tf\n",
    "import math as m\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras import layers\n",
    "from mxnet import nd\n",
    "import mxnet as mx\n",
    "from tensorflow import keras\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5322f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48993dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 0: 1152 samples\n",
      "Device 0:  (array([ 1,  6,  7,  8, 10], dtype=uint8), array([ 93, 383, 220, 229, 227]))\n",
      "Device 1: 1152 samples\n",
      "Device 1:  (array([ 1,  5,  7,  9, 10], dtype=uint8), array([119, 292, 284, 217, 240]))\n",
      "Device 2: 1152 samples\n",
      "Device 2:  (array([2, 3, 6, 8, 9], dtype=uint8), array([ 83, 127, 434, 274, 234]))\n",
      "Device 3: 1152 samples\n",
      "Device 3:  (array([ 0,  3,  4,  6, 10], dtype=uint8), array([125, 107, 276, 409, 235]))\n",
      "Device 4: 1152 samples\n",
      "Device 4:  (array([1, 4, 6, 7, 9], dtype=uint8), array([ 85, 243, 389, 251, 184]))\n",
      "Device 5: 1152 samples\n",
      "Device 5:  (array([2, 3, 4, 5, 9], dtype=uint8), array([106, 116, 335, 324, 271]))\n",
      "Device 6: 1152 samples\n",
      "Device 6:  (array([1, 5, 6, 8, 9], dtype=uint8), array([ 85, 226, 406, 233, 202]))\n",
      "Device 7: 1152 samples\n",
      "Device 7:  (array([0, 1, 2, 3, 4], dtype=uint8), array([231, 157, 160, 161, 443]))\n",
      "Device 8: 1152 samples\n",
      "Device 8:  (array([ 3,  4,  7,  9, 10], dtype=uint8), array([110, 304, 261, 215, 262]))\n",
      "Device 9: 1152 samples\n",
      "Device 9:  (array([0, 1, 2, 4, 9], dtype=uint8), array([210, 129, 161, 368, 284]))\n",
      "Device 10: 1152 samples\n",
      "Device 10:  (array([2, 3, 5, 7, 9], dtype=uint8), array([138, 116, 295, 340, 263]))\n",
      "Device 11: 1152 samples\n",
      "Device 11:  (array([1, 3, 6, 7, 8], dtype=uint8), array([ 89,  96, 437, 248, 282]))\n",
      "Device 12: 1152 samples\n",
      "Device 12:  (array([2, 4, 6, 7, 9], dtype=uint8), array([ 82, 263, 367, 241, 199]))\n",
      "Device 13: 1152 samples\n",
      "Device 13:  (array([2, 3, 6, 7, 9], dtype=uint8), array([105, 114, 440, 271, 222]))\n",
      "Device 14: 1152 samples\n",
      "Device 14:  (array([2, 3, 7, 8, 9], dtype=uint8), array([125, 109, 333, 334, 251]))\n",
      "Device 15: 1152 samples\n",
      "Device 15:  (array([ 0,  4,  7,  9, 10], dtype=uint8), array([141, 283, 278, 215, 235]))\n",
      "Device 16: 1152 samples\n",
      "Device 16:  (array([3, 6, 7, 8, 9], dtype=uint8), array([ 87, 385, 251, 255, 174]))\n",
      "Device 17: 1152 samples\n",
      "Device 17:  (array([ 4,  7,  8,  9, 10], dtype=uint8), array([230, 256, 238, 200, 228]))\n",
      "Device 18: 1152 samples\n",
      "Device 18:  (array([ 0,  2,  6,  7, 10], dtype=uint8), array([132,  87, 418, 268, 247]))\n",
      "Device 19: 1152 samples\n",
      "Device 19:  (array([0, 2, 3, 5, 7], dtype=uint8), array([177, 139, 147, 346, 343]))\n",
      "Device 20: 1152 samples\n",
      "Device 20:  (array([ 4,  7,  8,  9, 10], dtype=uint8), array([246, 234, 258, 188, 226]))\n",
      "Device 21: 1152 samples\n",
      "Device 21:  (array([ 2,  5,  8,  9, 10], dtype=uint8), array([100, 294, 275, 236, 247]))\n",
      "Device 22: 1152 samples\n",
      "Device 22:  (array([ 1,  4,  6,  7, 10], dtype=uint8), array([ 83, 230, 377, 243, 219]))\n",
      "Device 23: 1152 samples\n",
      "Device 23:  (array([ 2,  5,  6,  9, 10], dtype=uint8), array([ 80, 234, 380, 224, 234]))\n",
      "Device 24: 1152 samples\n",
      "Device 24:  (array([0, 1, 2, 4, 9], dtype=uint8), array([182, 135, 144, 423, 268]))\n",
      "Device 25: 1152 samples\n",
      "Device 25:  (array([ 1,  4,  6,  8, 10], dtype=uint8), array([ 90, 239, 393, 221, 209]))\n",
      "Device 26: 1152 samples\n",
      "Device 26:  (array([ 0,  1,  3,  4, 10], dtype=uint8), array([197, 117, 139, 356, 343]))\n",
      "Device 27: 1152 samples\n",
      "Device 27:  (array([1, 2, 4, 6, 9], dtype=uint8), array([109, 122, 270, 435, 216]))\n",
      "Device 28: 1152 samples\n",
      "Device 28:  (array([ 4,  6,  7,  9, 10], dtype=uint8), array([220, 341, 224, 170, 197]))\n",
      "Device 29: 1152 samples\n",
      "Device 29:  (array([ 0,  2,  3,  5, 10], dtype=uint8), array([184, 129, 134, 368, 337]))\n"
     ]
    }
   ],
   "source": [
    "data = np.load('/home/xipeng/FLcode/organamnist.npz')\n",
    "data.files\n",
    "\n",
    "x_train = data[\"train_images\"]\n",
    "y_train = data[\"train_labels\"]\n",
    "x_test = data[\"test_images\"]\n",
    "y_test = data[\"test_labels\"]\n",
    "\n",
    "# 为图像数据添加通道维度\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# 归一化图像数据\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "# 设备数量和IID程度\n",
    "num_devices = 30\n",
    "iid_ratio = 0.5\n",
    "\n",
    "# 计算每个设备应包含的类别数量\n",
    "num_classes = len(np.unique(y_train))\n",
    "samples_per_device = len(x_train) // num_devices\n",
    "selected_classes_per_device = int(num_classes * iid_ratio)\n",
    "\n",
    "# 分配数据\n",
    "device_data = [[] for _ in range(num_devices)]\n",
    "\n",
    "# 为每个设备分配数据\n",
    "for device_id in range(num_devices):\n",
    "    # 随机选择类别\n",
    "    random_classes = list(range(num_classes))\n",
    "    random.shuffle(random_classes)\n",
    "    selected_classes = random_classes[:selected_classes_per_device]\n",
    "\n",
    "    # 获取已选类别的索引\n",
    "    selected_indices = []\n",
    "    for cls in selected_classes:\n",
    "        class_indices = np.where(np.array(y_train) == cls)[0]\n",
    "        selected_indices.extend(class_indices)\n",
    "\n",
    "    # 在已选类别的索引中进行随机抽样\n",
    "    random_selected_indices = random.sample(selected_indices, samples_per_device)\n",
    "    device_data[device_id] = [(x_train[i], y_train[i]) for i in random_selected_indices]\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "# 检查每个设备上的样本数量\n",
    "for i in range(num_devices):\n",
    "    print(f\"Device {i}: {len(device_data[i])} samples\")\n",
    "    print(f\"Device {i}: \", np.unique(np.array([label for _, label in device_data[i]]), return_counts=True))  \n",
    "data_x = [np.array([sample for sample, _ in device_data[i]]) for i in range(num_devices)]\n",
    "data_y = [np.array([label for _, label in device_data[i]]) for i in range(num_devices)]\n",
    "data_y = keras.utils.to_categorical(data_y, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "x_train = data_x\n",
    "y_train = data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a342a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf836b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42989a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69f245b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  set_model():\n",
    "    model=Sequential()\n",
    "        # 定义顺序模型           \n",
    "    model.add(Convolution2D(\n",
    "        input_shape = (28,28,1),\n",
    "        filters = 8,\n",
    "        kernel_size = 3,\n",
    "        strides = 1,\n",
    "        padding = 'same',\n",
    "        activation = 'relu',\n",
    "    ))\n",
    "    # 第一个池化层\n",
    "    model.add(MaxPooling2D(\n",
    "        pool_size = 2,\n",
    "        strides = 2,\n",
    "        padding = 'same',\n",
    "    ))\n",
    "    # 第二个卷积层\n",
    "    model.add(Convolution2D(16,3,strides=1,padding='same',activation='relu'))\n",
    "    # 第二个池化层\n",
    "    model.add(MaxPooling2D(2,2,'same'))                                    \n",
    "                  \n",
    "       # 第二个卷积层      \n",
    "    # 把第三个池化层的输出扁平化为1维\n",
    "    model.add(layers.Flatten())\n",
    "    # 第一个全连接层\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    # Dropout\n",
    "    model.add(Dropout(0.5)) \n",
    "    # 第二个全连接层\n",
    "    model.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "    # 定义优化器\n",
    "    adam = Adam(lr =5e-4)        \n",
    "        \n",
    "    # 定义优化器,loss function,训练过程中计算准确率\n",
    "    model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "        #print(model_d[i])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d385711",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_parameters_gradients = []\n",
    "num_epochs = 20\n",
    "from keras import backend as K\n",
    "loss_function = CategoricalCrossentropy()\n",
    "def federated_learning(models,data_x, data_y,att_k):\n",
    "    client_parameters = []\n",
    "    client_gradients = []\n",
    "    \n",
    "        # 遍历每个模型\n",
    "    for i in range(len(models)):\n",
    "        client_models_parameters = []\n",
    "        client_models_gradients = []\n",
    "            # 使用客户端数据训练模型\n",
    "        models[i].fit(data_x[i], data_y[i],batch_size=16,epochs=3,verbose=0)\n",
    "            # 获取参数和梯度\n",
    "        weights = models[i].get_weights()\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = models[i](data_x[i], training=True)\n",
    "            loss = loss_function(data_y[i], predictions)\n",
    "        gradients = tape.gradient(loss, models[i].trainable_weights)\n",
    "\n",
    "            # 保存客户端的参数和梯度\n",
    "        client_models_parameters.append(weights)\n",
    "        client_models_gradients.append(gradients)\n",
    "\n",
    "        # 保存客户端的模型参数和梯度\n",
    "        client_parameters.append(client_models_parameters)\n",
    "        client_gradients.append(client_models_gradients)\n",
    "    for i in range(att_k):\n",
    "        for j in range(8):\n",
    "            client_parameters[i][0][j] = skimage.util.random_noise(client_parameters[i][0][j], mode=\"gaussian\", var=0.2, clip=True) \n",
    "    \n",
    "    # 聚合全局参数和梯度\n",
    "    aggregated_global_parameters = [np.mean([params[i] for params in client_parameters], axis=0) for i in range(len(client_parameters[0]))]\n",
    "    aggregated_global_gradients = [np.mean([grads[i] for grads in client_gradients], axis=0) for i in range(len(client_gradients[0]))]\n",
    "\n",
    "    return models,aggregated_global_parameters, aggregated_global_gradients,client_parameters,client_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bf7a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8633d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef55f2fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3bfd77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fedavg(aggregated_global_parameters,model_list):\n",
    "    for i in range (len(model_list)):\n",
    "        model_list[i].set_weights(aggregated_global_parameters[0])\n",
    "    loss,accuracy = model_list[0].evaluate(x_test,y_test,verbose=1)\n",
    "    return model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df427cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lbfgs(S_k_list, Y_k_list, v):\n",
    "    curr_S_k = nd.concat(*[nd.expand_dims(x, axis=1) for x in S_k_list], dim=1)\n",
    "    curr_Y_k = nd.concat(*[nd.expand_dims(x, axis=1) for x in Y_k_list], dim=1)\n",
    "\n",
    "    S_k_time_Y_k = nd.dot(curr_S_k.T, curr_Y_k)\n",
    "    S_k_time_S_k = nd.dot(curr_S_k.T, curr_S_k)\n",
    "    R_k = np.triu(S_k_time_Y_k.asnumpy())\n",
    "    \n",
    "    L_k = S_k_time_Y_k - nd.array(R_k, ctx=mx.cpu())\n",
    "    sigma_k = nd.dot(Y_k_list[-1].T, S_k_list[-1]) / (nd.dot(S_k_list[-1].T, S_k_list[-1]))\n",
    "    D_k_diag = nd.diag(S_k_time_Y_k)\n",
    "    upper_mat = nd.concat(*[sigma_k * S_k_time_S_k, L_k], dim=1)\n",
    "    lower_mat = nd.concat(*[L_k.T, -nd.diag(D_k_diag)], dim=1)\n",
    "    mat = nd.concat(*[upper_mat, lower_mat], dim=0)\n",
    "    mat_inv = nd.linalg.inverse(mat)\n",
    "    approx_prod = nd.array(sigma_k) * nd.array(v)\n",
    "    #approx_prod = sigma_k * v\n",
    "    #p_mat = nd.concat(*[nd.dot(curr_S_k.T, sigma_k * v), nd.dot(curr_Y_k.T, v)], dim=0)\n",
    "    p_mat = nd.concat(*[nd.dot(curr_S_k.T, sigma_k * nd.array(v)), nd.dot(curr_Y_k.T, nd.array(v))], dim=0)\n",
    "\n",
    "    approx_prod -= nd.dot(nd.dot(nd.concat(*[sigma_k * curr_S_k, curr_Y_k], dim=1), mat_inv), p_mat)\n",
    "\n",
    "    return approx_prod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdcb622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d50a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd, autograd, gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a01c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lbfgs(S_k_list, Y_k_list, v):\n",
    "#     gpu_device = 2  # 您可以根据需要修改这个值\n",
    "#     curr_S_k = nd.concat(*S_k_list, dim=1)\n",
    "#     curr_Y_k = nd.concat(*Y_k_list, dim=1)\n",
    "#     S_k_time_Y_k = nd.dot(curr_S_k.T, curr_Y_k)\n",
    "#     S_k_time_S_k = nd.dot(curr_S_k.T, curr_S_k)\n",
    "#     R_k = np.triu(S_k_time_Y_k.asnumpy())\n",
    "#     L_k = S_k_time_Y_k - nd.array(R_k, ctx=mx.gpu(gpu_device))\n",
    "#     sigma_k = nd.dot(Y_k_list[-1].T, S_k_list[-1]) / (nd.dot(S_k_list[-1].T, S_k_list[-1]))\n",
    "#     D_k_diag = nd.diag(S_k_time_Y_k)\n",
    "#     upper_mat = nd.concat(*[sigma_k * S_k_time_S_k, L_k], dim=1)\n",
    "#     lower_mat = nd.concat(*[L_k.T, -nd.diag(D_k_diag)], dim=1)\n",
    "#     mat = nd.concat(*[upper_mat, lower_mat], dim=0)\n",
    "#     mat_inv = nd.linalg.inverse(mat)\n",
    "\n",
    "#     approx_prod = sigma_k * v\n",
    "#     p_mat = nd.concat(*[nd.dot(curr_S_k.T, sigma_k * v), nd.dot(curr_Y_k.T, v)], dim=0)\n",
    "#     approx_prod -= nd.dot(nd.dot(nd.concat(*[sigma_k * curr_S_k, curr_Y_k], dim=1), mat_inv), p_mat)\n",
    "\n",
    "#     return approx_prod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78272e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "########写一个函数，该函数可以把数据拉伸\n",
    "def flattened_p(global_parameters):\n",
    "    global_parameters_flattened = np.concatenate([param.flatten() for param in global_parameters[0]])\n",
    "    return global_parameters_flattened\n",
    "\n",
    "def flattened_g(global_gradients):\n",
    "    global_gradients_numpy = [x.numpy() for x in global_gradients[0]]\n",
    "    global_gradients_numpy = np.array(global_gradients_numpy)\n",
    "    global_gradients_flattened = np.concatenate([param.flatten() for param in global_gradients_numpy])\n",
    "    return global_gradients_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c5436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b79ab455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fedavg_2(client_parameters,client_gradients,v,w_queue,g_queue,grad):\n",
    "    w_queue = [nd.array(x) for x in w_queue]\n",
    "    g_queue = [nd.array(x) for x in g_queue]\n",
    "    w_lsit= []\n",
    "    g_lsit = []\n",
    "    for i in range(10):\n",
    "        w_lsit.append(w_queue[i])\n",
    "        g_lsit.append(g_queue[i])\n",
    "    \n",
    "    \n",
    "    pre_c_gradients = []\n",
    "    pre_g = []\n",
    "    distances = []\n",
    "    for i in range(30):\n",
    "        hess = lbfgs(w_lsit, g_lsit, v[i])\n",
    "        hess_narry = hess.asnumpy()\n",
    "        grad_np = np.array(grad[i])\n",
    "        # 计算梯度之间的距离\n",
    "        distance = np.linalg.norm(grad_np - hess_narry)\n",
    "        distances.append(distance)\n",
    "    \n",
    "    \n",
    "    distances = np.array(distances).reshape(-1, 1)\n",
    "    # 假设我们想要将数据分为 3 个类别\n",
    "    k = 2\n",
    "\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(distances)\n",
    "\n",
    "    # kmeans.labels_ 将会告诉你每个数据点属于哪个类别\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "    # 找到数值最小的聚类中心的索引\n",
    "    min_cluster_index = np.argmin(cluster_centers)\n",
    "\n",
    "    # 找到所有属于该类别的索引\n",
    "    min_cluster_indices = np.where(kmeans.labels_ == min_cluster_index)[0]\n",
    "    \n",
    "    print(min_cluster_indices)\n",
    "    \n",
    "    filtered_client_parameters = [client_parameters[i] for i in min_cluster_indices]\n",
    "    filtered_client_gradients = [client_gradients[i] for i in min_cluster_indices]\n",
    "# 使用筛选后的客户端参数计算聚合的全局参数\n",
    "    \n",
    "    aggregated_global_parameters = [np.mean([params[i] for params in filtered_client_parameters], axis=0)for i in range(len(client_parameters[0]))]\n",
    "    aggregated_global_gradients = [np.mean([params[i] for params in filtered_client_gradients], axis=0)for i in range(len(client_gradients[0]))]\n",
    "    for i in range (30):\n",
    "        model_list[i].set_weights(aggregated_global_parameters[0])\n",
    "    \n",
    "    return model_list,aggregated_global_parameters,aggregated_global_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a5b961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b2d250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d132727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b701ad8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.deque'>\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "w_queue = deque([], maxlen=10)  # 创建一个长度为10的双向队列\n",
    "g_queue = deque([], maxlen=10)\n",
    "print(type(w_queue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deba9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9588737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c86e9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cfe12e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d566cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb851bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f05be54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 00:31:30.176180: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2023-07-04 00:31:30.223875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:86:00.0 name: NVIDIA Tesla V100-PCIE-16GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-07-04 00:31:30.223959: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-07-04 00:31:30.224069: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2023-07-04 00:31:30.224121: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2023-07-04 00:31:30.224178: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2023-07-04 00:31:30.224228: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-07-04 00:31:30.226375: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-07-04 00:31:30.226525: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-07-04 00:31:30.230594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2023-07-04 00:31:30.231365: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-04 00:31:30.247281: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 1700000000 Hz\n",
      "2023-07-04 00:31:30.249090: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5633d78e67e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-07-04 00:31:30.249146: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-07-04 00:31:30.548253: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5633d7952d00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-07-04 00:31:30.548341: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA Tesla V100-PCIE-16GB, Compute Capability 7.0\n",
      "2023-07-04 00:31:30.551680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:86:00.0 name: NVIDIA Tesla V100-PCIE-16GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-07-04 00:31:30.551769: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-07-04 00:31:30.551835: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2023-07-04 00:31:30.551878: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2023-07-04 00:31:30.551920: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2023-07-04 00:31:30.551962: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-07-04 00:31:30.552028: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-07-04 00:31:30.552070: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-07-04 00:31:30.557544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2023-07-04 00:31:30.557656: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-07-04 00:31:35.122101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-07-04 00:31:35.122177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
      "2023-07-04 00:31:35.122192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
      "2023-07-04 00:31:35.125989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14216 MB memory) -> physical GPU (device: 0, name: NVIDIA Tesla V100-PCIE-16GB, pci bus id: 0000:86:00.0, compute capability: 7.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 00:31:38.624034: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2023-07-04 00:31:39.050975: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-07-04 00:31:41.313802: W tensorflow/stream_executor/gpu/asm_compiler.cc:81] Running ptxas --version returned 256\n",
      "2023-07-04 00:31:41.543632: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556/556 [==============================] - 9s 16ms/step - loss: 2.3690 - accuracy: 0.0981\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 2.3426 - accuracy: 0.2297\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.7563 - accuracy: 0.3883\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.7458 - accuracy: 0.4035\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4900 - accuracy: 0.4522\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.9458 - accuracy: 0.3516\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4634 - accuracy: 0.5335\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3709 - accuracy: 0.5191\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.5576 - accuracy: 0.5042\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.5632 - accuracy: 0.5411\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2069 - accuracy: 0.6264\n",
      "0\n",
      "[ 1  4  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2625 - accuracy: 0.5854\n",
      "1\n",
      "[ 0  1  4  5  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1407 - accuracy: 0.6314\n",
      "2\n",
      "[ 0  2  6  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1944 - accuracy: 0.6611\n",
      "3\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.9366 - accuracy: 0.7035\n",
      "4\n",
      "[ 1  2  4  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0949 - accuracy: 0.6706\n",
      "5\n",
      "[ 2  4  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2823 - accuracy: 0.6369\n",
      "6\n",
      "[ 2  3  4  5  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0750 - accuracy: 0.6766\n",
      "7\n",
      "[ 0  1  3  5  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0378 - accuracy: 0.6575\n",
      "8\n",
      "[ 5  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.1955 - accuracy: 0.6586\n",
      "9\n",
      "[ 1  2  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3289 - accuracy: 0.5919\n",
      "10\n",
      "[ 4  5  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0753 - accuracy: 0.6606\n",
      "11\n",
      "[ 4  5  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3203 - accuracy: 0.5929\n",
      "12\n",
      "[ 0  3  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2149 - accuracy: 0.6668\n",
      "13\n",
      "[ 5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "  1/556 [..............................] - ETA: 0s - loss: 1.0750 - accuracy: 0.6562WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0021s vs `on_test_batch_end` time: 0.0037s). Check your callbacks.\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.9676 - accuracy: 0.7065\n",
      "14\n",
      "[ 5  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9095 - accuracy: 0.7231\n",
      "15\n",
      "[ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2197 - accuracy: 0.6333\n",
      "16\n",
      "[ 5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1021 - accuracy: 0.6888\n",
      "17\n",
      "[ 6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0246 - accuracy: 0.6993\n",
      "18\n",
      "[ 0  1  2  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.7112 - accuracy: 0.5016\n",
      "19\n",
      "[ 0  4  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.6831 - accuracy: 0.5065\n",
      "20\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2348 - accuracy: 0.6406\n",
      "21\n",
      "[ 0  1  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2941 - accuracy: 0.6358\n",
      "22\n",
      "[ 2  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9638 - accuracy: 0.7132\n",
      "23\n",
      "[ 0  3  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9753 - accuracy: 0.7157\n",
      "24\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4574 - accuracy: 0.5911\n",
      "25\n",
      "[ 0  1  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2424 - accuracy: 0.6959\n",
      "26\n",
      "[ 0  5  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1642 - accuracy: 0.6682\n",
      "27\n",
      "[ 7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3860 - accuracy: 0.6500\n",
      "28\n",
      "[ 6  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7659 - accuracy: 0.7503\n",
      "29\n",
      "[ 0  4  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1752 - accuracy: 0.6166\n",
      "30\n",
      "[ 5  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4320 - accuracy: 0.6488\n",
      "31\n",
      "[ 3  5  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4135 - accuracy: 0.6584\n",
      "32\n",
      "[ 7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.9433 - accuracy: 0.7305\n",
      "33\n",
      "[ 1  2  4  5  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8673 - accuracy: 0.7233\n",
      "34\n",
      "[ 0  2  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 1.1110 - accuracy: 0.6836\n",
      "35\n",
      "[ 1  2  3  5  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1392 - accuracy: 0.6938\n",
      "36\n",
      "[ 2  3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0359 - accuracy: 0.7062\n",
      "37\n",
      "[ 0  2  4  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0491 - accuracy: 0.7085\n",
      "38\n",
      "[ 0  1  2  3  4  5  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4157 - accuracy: 0.6207\n",
      "39\n",
      "[ 0  2  4  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.3755 - accuracy: 0.5565\n",
      "40\n",
      "[ 0  1  2  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9277 - accuracy: 0.7249\n",
      "41\n",
      "[ 1  2  3  5  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9865 - accuracy: 0.7239\n",
      "42\n",
      "[ 1  4  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9047 - accuracy: 0.7388\n",
      "43\n",
      "[ 0  1  4  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.1441 - accuracy: 0.6832\n",
      "44\n",
      "[ 0  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8917 - accuracy: 0.7298\n",
      "45\n",
      "[ 5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.0487 - accuracy: 0.7218\n",
      "46\n",
      "[ 2  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3341 - accuracy: 0.6768\n",
      "47\n",
      "[ 0  5  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8598 - accuracy: 0.7391\n",
      "48\n",
      "[ 2  3  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2095 - accuracy: 0.6897\n",
      "49\n",
      "[ 4  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8668 - accuracy: 0.7245\n",
      "50\n",
      "[ 0  2  4  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9516 - accuracy: 0.7238\n",
      "51\n",
      "[ 4  6  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9073 - accuracy: 0.7394\n",
      "52\n",
      "[ 0  2  3  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.0749 - accuracy: 0.7112\n",
      "53\n",
      "[ 2  4  5  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8490 - accuracy: 0.7351\n",
      "54\n",
      "[ 1  4  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8935 - accuracy: 0.6273\n",
      "55\n",
      "[ 0  1  5  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8362 - accuracy: 0.7414\n",
      "56\n",
      "[ 2  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0698 - accuracy: 0.6963\n",
      "57\n",
      "[ 2  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3480 - accuracy: 0.6520\n",
      "58\n",
      "[ 2  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1913 - accuracy: 0.6550\n",
      "59\n",
      "[ 0  1  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2388 - accuracy: 0.6984\n",
      "60\n",
      "[ 2  4  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4579 - accuracy: 0.6573\n",
      "61\n",
      "[ 2  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8260 - accuracy: 0.7574\n",
      "62\n",
      "[ 2  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.8619 - accuracy: 0.7410\n",
      "63\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7504 - accuracy: 0.7695\n",
      "64\n",
      "[ 0  1  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9325 - accuracy: 0.7025\n",
      "65\n",
      "[ 2  6  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7581 - accuracy: 0.7675\n",
      "66\n",
      "[ 0  2  4  5  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9510 - accuracy: 0.7404\n",
      "67\n",
      "[ 3  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8779 - accuracy: 0.7437\n",
      "68\n",
      "[ 1  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9373 - accuracy: 0.7382\n",
      "69\n",
      "[ 0  1  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9269 - accuracy: 0.7482\n",
      "70\n",
      "[ 1  3  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.6886 - accuracy: 0.6732\n",
      "71\n",
      "[ 0  1  4  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9126 - accuracy: 0.7373\n",
      "72\n",
      "[ 1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.6274 - accuracy: 0.6411\n",
      "73\n",
      "[ 0  4  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9265 - accuracy: 0.7344\n",
      "74\n",
      "[ 1  3  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8589 - accuracy: 0.7442\n",
      "75\n",
      "[ 0  3  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.8254 - accuracy: 0.7472\n",
      "76\n",
      "[ 3  4  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9220 - accuracy: 0.7291\n",
      "77\n",
      "[ 1  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.6911 - accuracy: 0.6466\n",
      "78\n",
      "[ 0  1  3  4  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8468 - accuracy: 0.7297\n",
      "79\n",
      "[ 0  3  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9583 - accuracy: 0.7382\n",
      "80\n",
      "[ 2  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7910 - accuracy: 0.7638\n",
      "81\n",
      "[ 0  2  3  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0349 - accuracy: 0.7262\n",
      "82\n",
      "[ 8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8443 - accuracy: 0.7324\n",
      "83\n",
      "[ 1  5  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8045 - accuracy: 0.7550\n",
      "84\n",
      "[ 0  4  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8516 - accuracy: 0.7328\n",
      "85\n",
      "[ 0  6  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.0556 - accuracy: 0.6223\n",
      "86\n",
      "[ 0  3  4  5  6  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4696 - accuracy: 0.6061\n",
      "87\n",
      "[ 2  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9391 - accuracy: 0.7350\n",
      "88\n",
      "[ 3  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8752 - accuracy: 0.7486\n",
      "89\n",
      "[ 1  2  3  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2332 - accuracy: 0.6421\n",
      "90\n",
      "[ 1  2  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2811 - accuracy: 0.6235\n",
      "91\n",
      "[ 1  2  4  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.0489 - accuracy: 0.7171\n",
      "92\n",
      "[ 2  3  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.8069 - accuracy: 0.7476\n",
      "93\n",
      "[ 1  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3583 - accuracy: 0.6947\n",
      "94\n",
      "[ 0  1  2  4  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.0767 - accuracy: 0.5938\n",
      "95\n",
      "[ 0  2  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8680 - accuracy: 0.7463\n",
      "96\n",
      "[ 2  6  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8397 - accuracy: 0.7450\n",
      "97\n",
      "[ 0  3  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0024 - accuracy: 0.7175\n",
      "98\n",
      "[ 0  2  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9344 - accuracy: 0.7385\n",
      "99\n",
      "[ 0  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0052 - accuracy: 0.6850\n",
      "100\n",
      "[ 3  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1086 - accuracy: 0.7077\n",
      "101\n",
      "[ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0847 - accuracy: 0.6993\n",
      "102\n",
      "[ 2  3  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1237 - accuracy: 0.6936\n",
      "103\n",
      "[ 1  2  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.8159 - accuracy: 0.7432\n",
      "104\n",
      "[ 5  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9273 - accuracy: 0.7302\n",
      "105\n",
      "[ 0  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8645 - accuracy: 0.7561\n",
      "106\n",
      "[ 0  1  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0771 - accuracy: 0.7043\n",
      "107\n",
      "[ 3  4  5  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0160 - accuracy: 0.7427\n",
      "108\n",
      "[ 1  3  4  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2995 - accuracy: 0.6987\n",
      "109\n",
      "[ 0  3  4  6  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2903 - accuracy: 0.6952\n",
      "110\n",
      "[ 0  1  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.3675 - accuracy: 0.6518\n",
      "111\n",
      "[ 1  5  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7679 - accuracy: 0.7618\n",
      "112\n",
      "[ 0  2  3  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8487 - accuracy: 0.7373\n",
      "113\n",
      "[ 0  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9081 - accuracy: 0.7510\n",
      "114\n",
      "[ 0  2  3  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3325 - accuracy: 0.6794\n",
      "115\n",
      "[ 5  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0374 - accuracy: 0.7285\n",
      "116\n",
      "[ 0  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7568 - accuracy: 0.7749\n",
      "117\n",
      "[ 1  3  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3626 - accuracy: 0.6936\n",
      "118\n",
      "[ 2  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9821 - accuracy: 0.7305\n",
      "119\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7746 - accuracy: 0.7708\n",
      "120\n",
      "[ 7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9504 - accuracy: 0.7432\n",
      "121\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8793 - accuracy: 0.7380\n",
      "122\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7831 - accuracy: 0.7685\n",
      "123\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7573 - accuracy: 0.7782\n",
      "124\n",
      "[ 1  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9082 - accuracy: 0.7284\n",
      "125\n",
      "[ 2  3  4  5  6  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8073 - accuracy: 0.7506\n",
      "126\n",
      "[ 0  1  2  3  4  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9967 - accuracy: 0.6992\n",
      "127\n",
      "[ 1  2  4  5  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 3.8153 - accuracy: 0.5341\n",
      "128\n",
      "[ 4  6  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7732 - accuracy: 0.7589\n",
      "129\n",
      "[ 1  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7969 - accuracy: 0.7598\n",
      "130\n",
      "[ 6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1486 - accuracy: 0.7155\n",
      "131\n",
      "[ 0  1  3  4  5  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0686 - accuracy: 0.7164\n",
      "132\n",
      "[ 0  3  4  5  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8729 - accuracy: 0.7342\n",
      "133\n",
      "[ 3  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3220 - accuracy: 0.6705\n",
      "134\n",
      "[ 7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7973 - accuracy: 0.7539\n",
      "135\n",
      "[ 2  6  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0739 - accuracy: 0.7180\n",
      "136\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7671 - accuracy: 0.7738\n",
      "137\n",
      "[ 1  2  4  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.5257 - accuracy: 0.6784\n",
      "138\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7672 - accuracy: 0.7803\n",
      "139\n",
      "[ 0  1  3  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7772 - accuracy: 0.7794\n",
      "140\n",
      "[ 0  1  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8454 - accuracy: 0.7642\n",
      "141\n",
      "[ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1161 - accuracy: 0.6657\n",
      "142\n",
      "[ 2  3  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9871 - accuracy: 0.7153\n",
      "143\n",
      "[ 0  2  3  5  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9096 - accuracy: 0.7422\n",
      "144\n",
      "[ 0  3  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9925 - accuracy: 0.7343\n",
      "145\n",
      "[ 3  4  6  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0790 - accuracy: 0.7257\n",
      "146\n",
      "[ 2  3  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9013 - accuracy: 0.7466\n",
      "147\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8051 - accuracy: 0.7685\n",
      "148\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7788 - accuracy: 0.7739\n",
      "149\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7864 - accuracy: 0.7749\n",
      "150\n",
      "[ 0  1  2  3  5  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9433 - accuracy: 0.7440\n",
      "151\n",
      "[ 7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9240 - accuracy: 0.7532\n",
      "152\n",
      "[ 0  1  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2657 - accuracy: 0.6914\n",
      "153\n",
      "[ 0  4  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9749 - accuracy: 0.7470\n",
      "154\n",
      "[ 2  4  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0518 - accuracy: 0.7449\n",
      "155\n",
      "[ 8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8010 - accuracy: 0.7680\n",
      "156\n",
      "[ 0  1  2  3  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1573 - accuracy: 0.6715\n",
      "157\n",
      "[ 1  5  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0139 - accuracy: 0.7312\n",
      "158\n",
      "[ 5  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7551 - accuracy: 0.7733\n",
      "159\n",
      "[ 0  1  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0308 - accuracy: 0.6650\n",
      "160\n",
      "[ 4  5  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9062 - accuracy: 0.7301\n",
      "161\n",
      "[ 3  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7400 - accuracy: 0.7606\n",
      "162\n",
      "[ 2  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0056 - accuracy: 0.7037\n",
      "163\n",
      "[ 0  2  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2477 - accuracy: 0.6423\n",
      "164\n",
      "[ 1  4  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1856 - accuracy: 0.6779\n",
      "165\n",
      "[ 4  5  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1347 - accuracy: 0.7032\n",
      "166\n",
      "[ 2  4  5  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2087 - accuracy: 0.6793\n",
      "167\n",
      "[ 1  2  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4603 - accuracy: 0.6680\n",
      "168\n",
      "[ 0  1  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9971 - accuracy: 0.7175\n",
      "169\n",
      "[ 1  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8377 - accuracy: 0.7573\n",
      "170\n",
      "[ 0  2  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0704 - accuracy: 0.6966\n",
      "171\n",
      "[ 2  3  4  5  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2657 - accuracy: 0.6907\n",
      "172\n",
      "[ 0  2  3  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9397 - accuracy: 0.7306\n",
      "173\n",
      "[ 1  3  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2696 - accuracy: 0.7027\n",
      "174\n",
      "[ 0  3  5  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.3858 - accuracy: 0.6160\n",
      "175\n",
      "[ 2  4  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8533 - accuracy: 0.7397\n",
      "176\n",
      "[ 1  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8024 - accuracy: 0.7432\n",
      "177\n",
      "[ 1  3  4  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8496 - accuracy: 0.7294\n",
      "178\n",
      "[ 3  4  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0428 - accuracy: 0.7221\n",
      "179\n",
      "[ 1  4  5  6  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0631 - accuracy: 0.7144\n",
      "180\n",
      "[ 7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7506 - accuracy: 0.7710\n",
      "181\n",
      "[ 1  2  4  5  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9168 - accuracy: 0.7433\n",
      "182\n",
      "[ 1  2  3  4  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.7136 - accuracy: 0.5048\n",
      "183\n",
      "[ 0  1  2  3  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0024 - accuracy: 0.7208\n",
      "184\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7161 - accuracy: 0.7749\n",
      "185\n",
      "[ 2  4  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9351 - accuracy: 0.7383\n",
      "186\n",
      "[ 6  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7339 - accuracy: 0.7676\n",
      "187\n",
      "[ 3  4  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9283 - accuracy: 0.7496\n",
      "188\n",
      "[ 3  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7713 - accuracy: 0.7689\n",
      "189\n",
      "[ 2  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2499 - accuracy: 0.7072\n",
      "190\n",
      "[ 2  3  4  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8458 - accuracy: 0.7528\n",
      "191\n",
      "[ 0  1  2  3  4  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2518 - accuracy: 0.6884\n",
      "192\n",
      "[ 0  1  2  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1576 - accuracy: 0.7094\n",
      "193\n",
      "[ 0  2  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9045 - accuracy: 0.7455\n",
      "194\n",
      "[ 0  1  2  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1724 - accuracy: 0.7125\n",
      "195\n",
      "[ 2  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7899 - accuracy: 0.7518\n",
      "196\n",
      "[ 0  1  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0826 - accuracy: 0.7146\n",
      "197\n",
      "[ 0  4  5  6  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1545 - accuracy: 0.6968\n",
      "198\n",
      "[ 2  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9298 - accuracy: 0.7513\n",
      "199\n",
      "[ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7687 - accuracy: 0.7735\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "w_queue = deque([], maxlen=10)  # 创建一个长度为10的双向队列\n",
    "g_queue = deque([], maxlen=10)\n",
    "global_parameters =[]\n",
    "global_gradients=[]\n",
    "client_parameters=[]\n",
    "client_gradients =[]\n",
    "loss_9 =[]\n",
    "att_k = 9\n",
    "\n",
    "model_list =[]\n",
    "for i in range(30):\n",
    "    model = set_model()\n",
    "    model_list.append(model)\n",
    "print(type(model_list))\n",
    "\n",
    "model_list,global_parameters, global_gradients,client_parameters,client_gradients = federated_learning(model_list,x_train,y_train,att_k)\n",
    "model_list = fedavg(global_parameters,model_list)\n",
    "\n",
    "global_gradients_flattened =  flattened_g(global_gradients)\n",
    "global_parameters_flattened =  flattened_p(global_parameters)\n",
    "last_global_gradients_flattened = global_gradients_flattened\n",
    "last_global_parameters_flattened = global_parameters_flattened\n",
    "\n",
    "\n",
    "\n",
    "for i in range (10):\n",
    "    model_list,global_parameters, global_gradients,client_parameters,client_gradients = federated_learning(model_list,x_train,y_train,att_k)\n",
    "    model_list = fedavg(global_parameters,model_list)\n",
    "    global_gradients_flattened =  flattened_g(global_gradients)\n",
    "    global_parameters_flattened =  flattened_p(global_parameters)\n",
    "    w_queue.append(global_parameters_flattened- last_global_parameters_flattened)\n",
    "    g_queue.append(global_gradients_flattened- last_global_gradients_flattened)\n",
    "    #########################这里的代码有问题，for i in range(len(global_parameters))，应该是一个，记得修改\n",
    "    last_global_gradients_flattened = global_gradients_flattened\n",
    "    last_global_parameters_flattened = global_parameters_flattened\n",
    "    #############################v = para - last para\n",
    "    last_para = []\n",
    "    last_grad = []\n",
    "    for i in range (30):\n",
    "        last_grad.append(flattened_g(client_gradients[i])) \n",
    "        last_para.append(flattened_p(client_parameters[i])) \n",
    "\n",
    "        \n",
    "for i in range (200):\n",
    "    model_list,global_parameters, global_gradients,client_parameters,client_gradients = federated_learning(model_list,x_train,y_train,att_k)\n",
    "    para = []\n",
    "    grad = []\n",
    "    print(i)\n",
    "    for i in range (30):\n",
    "        grad.append(flattened_g(client_gradients[i])) \n",
    "        para.append(flattened_p(client_parameters[i]))\n",
    "    v = [current - last for current, last in zip(para, last_para)]\n",
    "    model_list,global_parameters, global_gradients = fedavg_2(client_parameters,client_gradients,v,w_queue,g_queue,grad)\n",
    "    global_gradients_flattened =  flattened_g(global_gradients)\n",
    "    global_parameters_flattened =  flattened_p(global_parameters)\n",
    "    w_queue.append(global_parameters_flattened- last_global_parameters_flattened)\n",
    "    g_queue.append(global_gradients_flattened- last_global_gradients_flattened)\n",
    "    #########################这里的代码有问题，for i in range(len(global_parameters))，应该是一个，记得修改\n",
    "    last_global_gradients_flattened = global_gradients_flattened\n",
    "    last_global_parameters_flattened = global_parameters_flattened\n",
    "    #############################v = para - last para\n",
    "    last_para = para\n",
    "    last_grad = grad\n",
    "    loss,accuracy = model_list[0].evaluate(x_test,y_test,verbose=1)\n",
    "    loss_9.append(loss)\n",
    "    scipy.io.savemat('loss_9.mat', mdict={'loss_9': loss_9,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df19ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c48e029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.3972 - accuracy: 0.0574\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.2471 - accuracy: 0.2216\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8958 - accuracy: 0.3937\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.8514 - accuracy: 0.3316\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.6736 - accuracy: 0.4330\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.6384 - accuracy: 0.4633\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.5725 - accuracy: 0.4735\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.5377 - accuracy: 0.5145\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3347 - accuracy: 0.5686\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4448 - accuracy: 0.5436\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3967 - accuracy: 0.5705\n",
      "0\n",
      "[ 0  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3290 - accuracy: 0.5865\n",
      "1\n",
      "[ 2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2688 - accuracy: 0.5802\n",
      "2\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0721 - accuracy: 0.6648\n",
      "3\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0382 - accuracy: 0.6718\n",
      "4\n",
      "[ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0367 - accuracy: 0.6581\n",
      "5\n",
      "[ 1  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0650 - accuracy: 0.6608\n",
      "6\n",
      "[ 3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0245 - accuracy: 0.6817\n",
      "7\n",
      "[ 2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0585 - accuracy: 0.6619\n",
      "8\n",
      "[ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1905 - accuracy: 0.6428\n",
      "9\n",
      "[ 0  2  3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2108 - accuracy: 0.6482\n",
      "10\n",
      "[ 1  2  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2862 - accuracy: 0.5888\n",
      "11\n",
      "[ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9697 - accuracy: 0.6876\n",
      "12\n",
      "[ 1  3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9989 - accuracy: 0.6884\n",
      "13\n",
      "[ 3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9922 - accuracy: 0.6925\n",
      "14\n",
      "[ 4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1658 - accuracy: 0.6469\n",
      "15\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8828 - accuracy: 0.7172\n",
      "16\n",
      "[ 3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.8974 - accuracy: 0.7172\n",
      "17\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8861 - accuracy: 0.7211\n",
      "18\n",
      "[ 1  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9123 - accuracy: 0.7050\n",
      "19\n",
      "[ 4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9425 - accuracy: 0.7052\n",
      "20\n",
      "[ 4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9774 - accuracy: 0.7122\n",
      "21\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 1s 2ms/step - loss: 0.8419 - accuracy: 0.7346\n",
      "22\n",
      "[ 0  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9216 - accuracy: 0.7143\n",
      "23\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8226 - accuracy: 0.7417\n",
      "24\n",
      "[ 3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9809 - accuracy: 0.7045\n",
      "25\n",
      "[ 2  3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8847 - accuracy: 0.7246\n",
      "26\n",
      "[ 4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9547 - accuracy: 0.7162\n",
      "27\n",
      "[ 1  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8435 - accuracy: 0.7333\n",
      "28\n",
      "[ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8861 - accuracy: 0.7215\n",
      "29\n",
      "[ 0  2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8615 - accuracy: 0.7366\n",
      "30\n",
      "[ 1  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8954 - accuracy: 0.7368\n",
      "31\n",
      "[ 1  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8055 - accuracy: 0.7478\n",
      "32\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7974 - accuracy: 0.7530\n",
      "33\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7733 - accuracy: 0.7595\n",
      "34\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7698 - accuracy: 0.7625\n",
      "35\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8874 - accuracy: 0.7181\n",
      "36\n",
      "[ 1  3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9393 - accuracy: 0.7370\n",
      "37\n",
      "[ 1  2  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9581 - accuracy: 0.7187\n",
      "38\n",
      "[ 1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8519 - accuracy: 0.7337\n",
      "39\n",
      "[ 0  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8870 - accuracy: 0.7315\n",
      "40\n",
      "[ 4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8873 - accuracy: 0.7417\n",
      "41\n",
      "[ 1  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7718 - accuracy: 0.7598\n",
      "42\n",
      "[ 2  3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8401 - accuracy: 0.7512\n",
      "43\n",
      "[ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1555 - accuracy: 0.7021\n",
      "44\n",
      "[ 1  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7572 - accuracy: 0.7692\n",
      "45\n",
      "[ 1  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8826 - accuracy: 0.7434\n",
      "46\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7689 - accuracy: 0.7717\n",
      "47\n",
      "[ 0  1  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9658 - accuracy: 0.7203\n",
      "48\n",
      "[ 1  2  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8686 - accuracy: 0.7420\n",
      "49\n",
      "[ 3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9275 - accuracy: 0.7398\n",
      "50\n",
      "[ 0  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9152 - accuracy: 0.7338\n",
      "51\n",
      "[ 3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8284 - accuracy: 0.7589\n",
      "52\n",
      "[ 2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0098 - accuracy: 0.7289\n",
      "53\n",
      "[ 2  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7688 - accuracy: 0.7611\n",
      "54\n",
      "[ 4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8855 - accuracy: 0.7499\n",
      "55\n",
      "[ 0  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8796 - accuracy: 0.7564\n",
      "56\n",
      "[ 0  1  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0118 - accuracy: 0.7249\n",
      "57\n",
      "[ 0  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.9081 - accuracy: 0.7533\n",
      "58\n",
      "[ 0  1  2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7839 - accuracy: 0.7705\n",
      "59\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7357 - accuracy: 0.7782\n",
      "60\n",
      "[ 1  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9872 - accuracy: 0.7393\n",
      "61\n",
      "[ 1  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.7593 - accuracy: 0.7765\n",
      "62\n",
      "[ 0  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9676 - accuracy: 0.7423\n",
      "63\n",
      "[ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8609 - accuracy: 0.7528\n",
      "64\n",
      "[ 0  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8398 - accuracy: 0.7680\n",
      "65\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2974 - accuracy: 0.6802\n",
      "66\n",
      "[ 0  1  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8816 - accuracy: 0.7361\n",
      "67\n",
      "[ 0  1  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7019 - accuracy: 0.7799\n",
      "68\n",
      "[ 4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9736 - accuracy: 0.7440\n",
      "69\n",
      "[ 2  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0084 - accuracy: 0.7168\n",
      "70\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7286 - accuracy: 0.7806\n",
      "71\n",
      "[ 3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7752 - accuracy: 0.7779\n",
      "72\n",
      "[ 3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7356 - accuracy: 0.7798\n",
      "73\n",
      "[ 3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7895 - accuracy: 0.7751\n",
      "74\n",
      "[ 1  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8395 - accuracy: 0.7673\n",
      "75\n",
      "[ 0  1  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.6936 - accuracy: 0.7634\n",
      "76\n",
      "[ 1  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.6945 - accuracy: 0.7842\n",
      "77\n",
      "[ 1  2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.7188 - accuracy: 0.7777\n",
      "78\n",
      "[ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8085 - accuracy: 0.7486\n",
      "79\n",
      "[ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7540 - accuracy: 0.7604\n",
      "80\n",
      "[ 2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7351 - accuracy: 0.7795\n",
      "81\n",
      "[ 3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2390 - accuracy: 0.7190\n",
      "82\n",
      "[ 1  2  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0157 - accuracy: 0.7219\n",
      "83\n",
      "[ 4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8281 - accuracy: 0.7570\n",
      "84\n",
      "[ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.7920 - accuracy: 0.7727\n",
      "85\n",
      "[ 0  1  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7370 - accuracy: 0.7770\n",
      "86\n",
      "[ 2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.8704 - accuracy: 0.7620\n",
      "87\n",
      "[ 0  2  3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8481 - accuracy: 0.7472\n",
      "88\n",
      "[ 1  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8473 - accuracy: 0.7675\n",
      "89\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7142 - accuracy: 0.7873\n",
      "90\n",
      "[ 3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8765 - accuracy: 0.7428\n",
      "91\n",
      "[ 3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9805 - accuracy: 0.7551\n",
      "92\n",
      "[ 4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9444 - accuracy: 0.7397\n",
      "93\n",
      "[ 2  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8440 - accuracy: 0.7571\n",
      "94\n",
      "[ 0  1  2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9285 - accuracy: 0.7083\n",
      "95\n",
      "[ 1  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.6688 - accuracy: 0.7913\n",
      "96\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7116 - accuracy: 0.7935\n",
      "97\n",
      "[ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1237 - accuracy: 0.7312\n",
      "98\n",
      "[ 1  2  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0747 - accuracy: 0.7355\n",
      "99\n",
      "[ 2  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7683 - accuracy: 0.7757\n",
      "100\n",
      "[ 2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.6703 - accuracy: 0.7963\n",
      "101\n",
      "[ 4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7289 - accuracy: 0.7753\n",
      "102\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7024 - accuracy: 0.7964\n",
      "103\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.6917 - accuracy: 0.8003\n",
      "104\n",
      "[ 0  3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8900 - accuracy: 0.7676\n",
      "105\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7659 - accuracy: 0.7571\n",
      "106\n",
      "[ 1  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7689 - accuracy: 0.7594\n",
      "107\n",
      "[ 0  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0318 - accuracy: 0.7431\n",
      "108\n",
      "[ 1  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.7578 - accuracy: 0.7758\n",
      "109\n",
      "[ 3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.8825 - accuracy: 0.7392\n",
      "110\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.6786 - accuracy: 0.7998\n",
      "111\n",
      "[ 1  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7586 - accuracy: 0.7874\n",
      "112\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.6850 - accuracy: 0.8006\n",
      "113\n",
      "[ 1  2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7440 - accuracy: 0.7937\n",
      "114\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.6883 - accuracy: 0.8020\n",
      "115\n",
      "[ 0  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8450 - accuracy: 0.7422\n",
      "116\n",
      "[ 1  3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8336 - accuracy: 0.7643\n",
      "117\n",
      "[ 3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7170 - accuracy: 0.7800\n",
      "118\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.6674 - accuracy: 0.8031\n",
      "119\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.6666 - accuracy: 0.8046\n",
      "120\n",
      "[ 0  1  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8032 - accuracy: 0.7879\n",
      "121\n",
      "[ 0  1  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8874 - accuracy: 0.7378\n",
      "122\n",
      "[ 0  1  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1503 - accuracy: 0.7453\n",
      "123\n",
      "[ 3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.6727 - accuracy: 0.7999\n",
      "124\n",
      "[ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7206 - accuracy: 0.7908\n",
      "125\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.6682 - accuracy: 0.8015\n",
      "126\n",
      "[ 3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.6601 - accuracy: 0.8008\n",
      "127\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7858 - accuracy: 0.7814\n",
      "128\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8725 - accuracy: 0.7544\n",
      "129\n",
      "[ 0  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8059 - accuracy: 0.7615\n",
      "130\n",
      "[ 3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7375 - accuracy: 0.7890\n",
      "131\n",
      "[ 0  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8683 - accuracy: 0.7641\n",
      "132\n",
      "[ 1  2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8404 - accuracy: 0.7610\n",
      "133\n",
      "[ 3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8233 - accuracy: 0.7561\n",
      "134\n",
      "[ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8473 - accuracy: 0.7694\n",
      "135\n",
      "[ 1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8591 - accuracy: 0.7554\n",
      "136\n",
      "[ 0  1  2  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8971 - accuracy: 0.7539\n",
      "137\n",
      "[ 1  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7169 - accuracy: 0.7900\n",
      "138\n",
      "[ 0  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0478 - accuracy: 0.7429\n",
      "139\n",
      "[ 1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8597 - accuracy: 0.7515\n",
      "140\n",
      "[ 1  2  3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7943 - accuracy: 0.7813\n",
      "141\n",
      "[ 3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0281 - accuracy: 0.7443\n",
      "142\n",
      "[ 3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8807 - accuracy: 0.7524\n",
      "143\n",
      "[ 1  3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7572 - accuracy: 0.7805\n",
      "144\n",
      "[ 2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1211 - accuracy: 0.7333\n",
      "145\n",
      "[ 0  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.7520 - accuracy: 0.7837\n",
      "146\n",
      "[ 0  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9490 - accuracy: 0.7621\n",
      "147\n",
      "[ 0  1  2  3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9457 - accuracy: 0.7295\n",
      "148\n",
      "[ 1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9322 - accuracy: 0.7078\n",
      "149\n",
      "[ 0  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.8177 - accuracy: 0.7664\n",
      "150\n",
      "[ 0  1  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8675 - accuracy: 0.7789\n",
      "151\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.6877 - accuracy: 0.8008\n",
      "152\n",
      "[ 0  2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8985 - accuracy: 0.7680\n",
      "153\n",
      "[ 2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.6789 - accuracy: 0.8041\n",
      "154\n",
      "[ 2  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7908 - accuracy: 0.7822\n",
      "155\n",
      "[ 2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7566 - accuracy: 0.7886\n",
      "156\n",
      "[ 2  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7896 - accuracy: 0.7644\n",
      "157\n",
      "[ 3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8987 - accuracy: 0.7663\n",
      "158\n",
      "[ 1  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7790 - accuracy: 0.7781\n",
      "159\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.6992 - accuracy: 0.8001\n",
      "160\n",
      "[ 0  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.8759 - accuracy: 0.7838\n",
      "161\n",
      "[ 0  2  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0283 - accuracy: 0.7491\n",
      "162\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.6650 - accuracy: 0.8084\n",
      "163\n",
      "[ 4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8272 - accuracy: 0.7821\n",
      "164\n",
      "[ 2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8551 - accuracy: 0.7524\n",
      "165\n",
      "[ 2  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8180 - accuracy: 0.7842\n",
      "166\n",
      "[ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8814 - accuracy: 0.7469\n",
      "167\n",
      "[ 0  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8209 - accuracy: 0.7581\n",
      "168\n",
      "[ 0  1  3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1700 - accuracy: 0.7402\n",
      "169\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.6851 - accuracy: 0.7983\n",
      "170\n",
      "[ 0  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7016 - accuracy: 0.7965\n",
      "171\n",
      "[ 0  2  3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7447 - accuracy: 0.7801\n",
      "172\n",
      "[ 2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.8167 - accuracy: 0.7758\n",
      "173\n",
      "[ 1  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8691 - accuracy: 0.7748\n",
      "174\n",
      "[ 0  1  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7692 - accuracy: 0.7771\n",
      "175\n",
      "[ 3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7137 - accuracy: 0.7761\n",
      "176\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7178 - accuracy: 0.7873\n",
      "177\n",
      "[ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7676 - accuracy: 0.7847\n",
      "178\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.6817 - accuracy: 0.7975\n",
      "179\n",
      "[ 0  3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8991 - accuracy: 0.7647\n",
      "180\n",
      "[ 2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9102 - accuracy: 0.7608\n",
      "181\n",
      "[ 3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.6917 - accuracy: 0.7864\n",
      "182\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.6811 - accuracy: 0.7978\n",
      "183\n",
      "[ 2  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8180 - accuracy: 0.7445\n",
      "184\n",
      "[ 1  2  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7541 - accuracy: 0.7780\n",
      "185\n",
      "[ 2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7017 - accuracy: 0.7844\n",
      "186\n",
      "[ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.5993 - accuracy: 0.6857\n",
      "187\n",
      "[ 2  3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7993 - accuracy: 0.7738\n",
      "188\n",
      "[ 0  3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7275 - accuracy: 0.7875\n",
      "189\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.6731 - accuracy: 0.7983\n",
      "190\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.6668 - accuracy: 0.8014\n",
      "191\n",
      "[ 1  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.7922 - accuracy: 0.7856\n",
      "192\n",
      "[ 0  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0168 - accuracy: 0.7275\n",
      "193\n",
      "[ 0  2  3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1151 - accuracy: 0.7044\n",
      "194\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.5134 - accuracy: 0.6966\n",
      "195\n",
      "[ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8099 - accuracy: 0.7667\n",
      "196\n",
      "[ 2  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28\n",
      " 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7816 - accuracy: 0.7827\n",
      "197\n",
      "[ 0  1  2  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8610 - accuracy: 0.7532\n",
      "198\n",
      "[ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2635 - accuracy: 0.6911\n",
      "199\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.6908 - accuracy: 0.7942\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "w_queue = deque([], maxlen=10)  # 创建一个长度为10的双向队列\n",
    "g_queue = deque([], maxlen=10)\n",
    "global_parameters =[]\n",
    "global_gradients=[]\n",
    "client_parameters=[]\n",
    "client_gradients =[]\n",
    "loss_6 =[]\n",
    "att_k = 6\n",
    "\n",
    "model_list =[]\n",
    "for i in range(30):\n",
    "    model = set_model()\n",
    "    model_list.append(model)\n",
    "print(type(model_list))\n",
    "\n",
    "model_list,global_parameters, global_gradients,client_parameters,client_gradients = federated_learning(model_list,x_train,y_train,att_k)\n",
    "model_list = fedavg(global_parameters,model_list)\n",
    "\n",
    "global_gradients_flattened =  flattened_g(global_gradients)\n",
    "global_parameters_flattened =  flattened_p(global_parameters)\n",
    "last_global_gradients_flattened = global_gradients_flattened\n",
    "last_global_parameters_flattened = global_parameters_flattened\n",
    "\n",
    "\n",
    "\n",
    "for i in range (10):\n",
    "    model_list,global_parameters, global_gradients,client_parameters,client_gradients = federated_learning(model_list,x_train,y_train,att_k)\n",
    "    model_list = fedavg(global_parameters,model_list)\n",
    "    global_gradients_flattened =  flattened_g(global_gradients)\n",
    "    global_parameters_flattened =  flattened_p(global_parameters)\n",
    "    w_queue.append(global_parameters_flattened- last_global_parameters_flattened)\n",
    "    g_queue.append(global_gradients_flattened- last_global_gradients_flattened)\n",
    "    #########################这里的代码有问题，for i in range(len(global_parameters))，应该是一个，记得修改\n",
    "    last_global_gradients_flattened = global_gradients_flattened\n",
    "    last_global_parameters_flattened = global_parameters_flattened\n",
    "    #############################v = para - last para\n",
    "    last_para = []\n",
    "    last_grad = []\n",
    "    for i in range (30):\n",
    "        last_grad.append(flattened_g(client_gradients[i])) \n",
    "        last_para.append(flattened_p(client_parameters[i])) \n",
    "\n",
    "        \n",
    "for i in range (200):\n",
    "    model_list,global_parameters, global_gradients,client_parameters,client_gradients = federated_learning(model_list,x_train,y_train,att_k)\n",
    "    para = []\n",
    "    grad = []\n",
    "    print(i)\n",
    "    for i in range (30):\n",
    "        grad.append(flattened_g(client_gradients[i])) \n",
    "        para.append(flattened_p(client_parameters[i]))\n",
    "    v = [current - last for current, last in zip(para, last_para)]\n",
    "    model_list,global_parameters, global_gradients = fedavg_2(client_parameters,client_gradients,v,w_queue,g_queue,grad)\n",
    "    global_gradients_flattened =  flattened_g(global_gradients)\n",
    "    global_parameters_flattened =  flattened_p(global_parameters)\n",
    "    w_queue.append(global_parameters_flattened- last_global_parameters_flattened)\n",
    "    g_queue.append(global_gradients_flattened- last_global_gradients_flattened)\n",
    "    #########################这里的代码有问题，for i in range(len(global_parameters))，应该是一个，记得修改\n",
    "    last_global_gradients_flattened = global_gradients_flattened\n",
    "    last_global_parameters_flattened = global_parameters_flattened\n",
    "    #############################v = para - last para\n",
    "    last_para = para\n",
    "    last_grad = grad \n",
    "    loss,accuracy = model_list[0].evaluate(x_test,y_test,verbose=1)\n",
    "    loss_6.append(loss)\n",
    "    scipy.io.savemat('loss_6.mat', mdict={'loss_6': loss_6,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c47890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93364231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#最后看一下c_gradients_flattened和pre_c_gradients的差值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57681d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.4007 - accuracy: 0.0912\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 2.1485 - accuracy: 0.2964\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 1.9134 - accuracy: 0.4296\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.7926 - accuracy: 0.4228\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.7396 - accuracy: 0.4822\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.6895 - accuracy: 0.4552\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.6192 - accuracy: 0.4908\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.5349 - accuracy: 0.5192\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4635 - accuracy: 0.5600\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4251 - accuracy: 0.5366\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3521 - accuracy: 0.5631\n",
      "0\n",
      "[ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3787 - accuracy: 0.5666\n",
      "1\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3226 - accuracy: 0.5723\n",
      "2\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.3853 - accuracy: 0.5563\n",
      "3\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2013 - accuracy: 0.6127\n",
      "4\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 1.1605 - accuracy: 0.6261\n",
      "5\n",
      "[ 0  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1073 - accuracy: 0.6447\n",
      "6\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 1.1026 - accuracy: 0.6519\n",
      "7\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0664 - accuracy: 0.6601\n",
      "8\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0519 - accuracy: 0.6702\n",
      "9\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0213 - accuracy: 0.6810\n",
      "10\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1405 - accuracy: 0.6621\n",
      "11\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0863 - accuracy: 0.6536\n",
      "12\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0046 - accuracy: 0.6918\n",
      "13\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9907 - accuracy: 0.6928\n",
      "14\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9638 - accuracy: 0.7023\n",
      "15\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "  1/556 [..............................] - ETA: 0s - loss: 1.1642 - accuracy: 0.6562WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_test_batch_end` time: 0.0026s). Check your callbacks.\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 1.1299 - accuracy: 0.6691\n",
      "16\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1313 - accuracy: 0.6653\n",
      "17\n",
      "[ 0  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.9317 - accuracy: 0.7111\n",
      "18\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.9429 - accuracy: 0.7161\n",
      "19\n",
      "[ 1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8970 - accuracy: 0.7202\n",
      "20\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9064 - accuracy: 0.7135\n",
      "21\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8963 - accuracy: 0.7274\n",
      "22\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8835 - accuracy: 0.7283\n",
      "23\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9433 - accuracy: 0.7242\n",
      "24\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8849 - accuracy: 0.7328\n",
      "25\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1264 - accuracy: 0.6928\n",
      "26\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8495 - accuracy: 0.7406\n",
      "27\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9556 - accuracy: 0.7225\n",
      "28\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8515 - accuracy: 0.7402\n",
      "29\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8739 - accuracy: 0.7356\n",
      "30\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8489 - accuracy: 0.7439\n",
      "31\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1408 - accuracy: 0.6458\n",
      "32\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8319 - accuracy: 0.7425\n",
      "33\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8247 - accuracy: 0.7493\n",
      "34\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8222 - accuracy: 0.7523\n",
      "35\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8021 - accuracy: 0.7570\n",
      "36\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8073 - accuracy: 0.7553\n",
      "37\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2267 - accuracy: 0.6956\n",
      "38\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7924 - accuracy: 0.7545\n",
      "39\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8179 - accuracy: 0.7562\n",
      "40\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7852 - accuracy: 0.7609\n",
      "41\n",
      "[ 1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.8029 - accuracy: 0.7582\n",
      "42\n",
      "[ 1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8343 - accuracy: 0.7490\n",
      "43\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7899 - accuracy: 0.7634\n",
      "44\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9246 - accuracy: 0.7172\n",
      "45\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7960 - accuracy: 0.7598\n",
      "46\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7859 - accuracy: 0.7656\n",
      "47\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7862 - accuracy: 0.7686\n",
      "48\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8026 - accuracy: 0.7629\n",
      "49\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8775 - accuracy: 0.7575\n",
      "50\n",
      "[ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.0835 - accuracy: 0.7209\n",
      "51\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.7827 - accuracy: 0.7532\n",
      "52\n",
      "[ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8090 - accuracy: 0.7625\n",
      "53\n",
      "[ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 1.0354 - accuracy: 0.7269\n",
      "54\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.7830 - accuracy: 0.7665\n",
      "55\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7735 - accuracy: 0.7706\n",
      "56\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7648 - accuracy: 0.7735\n",
      "57\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7515 - accuracy: 0.7703\n",
      "58\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.9811 - accuracy: 0.7413\n",
      "59\n",
      "[ 1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7555 - accuracy: 0.7731\n",
      "60\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7287 - accuracy: 0.7659\n",
      "61\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8271 - accuracy: 0.7640\n",
      "62\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7459 - accuracy: 0.7776\n",
      "63\n",
      "[ 0  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7718 - accuracy: 0.7695\n",
      "64\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.7456 - accuracy: 0.7816\n",
      "65\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7798 - accuracy: 0.7676\n",
      "66\n",
      "[ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7643 - accuracy: 0.7727\n",
      "67\n",
      "[ 1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9291 - accuracy: 0.7583\n",
      "68\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7124 - accuracy: 0.7832\n",
      "69\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7270 - accuracy: 0.7829\n",
      "70\n",
      "[ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.2472 - accuracy: 0.6978\n",
      "71\n",
      "[ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7101 - accuracy: 0.7841\n",
      "72\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9029 - accuracy: 0.7575\n",
      "73\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7289 - accuracy: 0.7856\n",
      "74\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7929 - accuracy: 0.7708\n",
      "75\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7173 - accuracy: 0.7852\n",
      "76\n",
      "[ 1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7126 - accuracy: 0.7923\n",
      "77\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8052 - accuracy: 0.7630\n",
      "78\n",
      "[ 1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.7121 - accuracy: 0.7874\n",
      "79\n",
      "[ 1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7994 - accuracy: 0.7650\n",
      "80\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7441 - accuracy: 0.7852\n",
      "81\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8634 - accuracy: 0.7411\n",
      "82\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7205 - accuracy: 0.7919\n",
      "83\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7124 - accuracy: 0.7941\n",
      "84\n",
      "[ 0  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8984 - accuracy: 0.7774\n",
      "85\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7245 - accuracy: 0.7923\n",
      "86\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7571 - accuracy: 0.7721\n",
      "87\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9648 - accuracy: 0.7559\n",
      "88\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7848 - accuracy: 0.7763\n",
      "89\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9359 - accuracy: 0.7602\n",
      "90\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7580 - accuracy: 0.7846\n",
      "91\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7482 - accuracy: 0.7917\n",
      "92\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7451 - accuracy: 0.7914\n",
      "93\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8557 - accuracy: 0.7724\n",
      "94\n",
      "[ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9547 - accuracy: 0.7721\n",
      "95\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7726 - accuracy: 0.7732\n",
      "96\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9291 - accuracy: 0.7580\n",
      "97\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7694 - accuracy: 0.7668\n",
      "98\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.8338 - accuracy: 0.7676\n",
      "99\n",
      "[ 1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.8443 - accuracy: 0.7756\n",
      "100\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7110 - accuracy: 0.7968\n",
      "101\n",
      "[ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.4083 - accuracy: 0.7062\n",
      "102\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7207 - accuracy: 0.7956\n",
      "103\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7384 - accuracy: 0.7941\n",
      "104\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.7327 - accuracy: 0.7945\n",
      "105\n",
      "[ 0  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7844 - accuracy: 0.7901\n",
      "106\n",
      "[ 0  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8178 - accuracy: 0.7805\n",
      "107\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7515 - accuracy: 0.7900\n",
      "108\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7331 - accuracy: 0.7956\n",
      "109\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.7378 - accuracy: 0.7959\n",
      "110\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8360 - accuracy: 0.7769\n",
      "111\n",
      "[ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8729 - accuracy: 0.7813\n",
      "112\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7176 - accuracy: 0.7924\n",
      "113\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8240 - accuracy: 0.7799\n",
      "114\n",
      "[ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7976 - accuracy: 0.7621\n",
      "115\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.7180 - accuracy: 0.7951\n",
      "116\n",
      "[ 0  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7816 - accuracy: 0.7801\n",
      "117\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7318 - accuracy: 0.7955\n",
      "118\n",
      "[ 1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9108 - accuracy: 0.7793\n",
      "119\n",
      "[ 0  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7640 - accuracy: 0.7953\n",
      "120\n",
      "[ 0  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 1.1085 - accuracy: 0.7617\n",
      "121\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.6997 - accuracy: 0.7977\n",
      "122\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.7304 - accuracy: 0.7965\n",
      "123\n",
      "[ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8808 - accuracy: 0.7797\n",
      "124\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8406 - accuracy: 0.7869\n",
      "125\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 1s 2ms/step - loss: 0.7142 - accuracy: 0.7978\n",
      "126\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8297 - accuracy: 0.7746\n",
      "127\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.7754 - accuracy: 0.7924\n",
      "128\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9345 - accuracy: 0.7663\n",
      "129\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7302 - accuracy: 0.7983\n",
      "130\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7138 - accuracy: 0.7977\n",
      "131\n",
      "[ 0  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7937 - accuracy: 0.7877\n",
      "132\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7218 - accuracy: 0.7995\n",
      "133\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7797 - accuracy: 0.7946\n",
      "134\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7558 - accuracy: 0.7954\n",
      "135\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7303 - accuracy: 0.8004\n",
      "136\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.7298 - accuracy: 0.8014\n",
      "137\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7206 - accuracy: 0.8050\n",
      "138\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 4ms/step - loss: 0.6920 - accuracy: 0.8064\n",
      "139\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7186 - accuracy: 0.8014\n",
      "140\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7001 - accuracy: 0.8063\n",
      "141\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7995 - accuracy: 0.7942\n",
      "142\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7346 - accuracy: 0.7892\n",
      "143\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7319 - accuracy: 0.8042\n",
      "144\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7199 - accuracy: 0.8066\n",
      "145\n",
      "[ 1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8553 - accuracy: 0.7911\n",
      "146\n",
      "[ 0  3  4  6  7  8  9 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.7804 - accuracy: 0.7933\n",
      "147\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7071 - accuracy: 0.8090\n",
      "148\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7631 - accuracy: 0.7986\n",
      "149\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7009 - accuracy: 0.7969\n",
      "150\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7251 - accuracy: 0.8045\n",
      "151\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7408 - accuracy: 0.8017\n",
      "152\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7392 - accuracy: 0.8048\n",
      "153\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7358 - accuracy: 0.8059\n",
      "154\n",
      "[ 1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.7506 - accuracy: 0.8050\n",
      "155\n",
      "[ 1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7920 - accuracy: 0.8007\n",
      "156\n",
      "[ 1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.7588 - accuracy: 0.8019\n",
      "157\n",
      "[ 1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7616 - accuracy: 0.8010\n",
      "158\n",
      "[ 1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8630 - accuracy: 0.7944\n",
      "159\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7109 - accuracy: 0.8060\n",
      "160\n",
      "[ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8679 - accuracy: 0.7865\n",
      "161\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.6830 - accuracy: 0.8136\n",
      "162\n",
      "[ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8802 - accuracy: 0.7900\n",
      "163\n",
      "[ 0  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7113 - accuracy: 0.8109\n",
      "164\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7715 - accuracy: 0.7979\n",
      "165\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.6976 - accuracy: 0.8120\n",
      "166\n",
      "[ 0  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7849 - accuracy: 0.8024\n",
      "167\n",
      "[ 0  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7471 - accuracy: 0.8101\n",
      "168\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7017 - accuracy: 0.8157\n",
      "169\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7379 - accuracy: 0.8055\n",
      "170\n",
      "[ 0  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.8344 - accuracy: 0.7955\n",
      "171\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7285 - accuracy: 0.8107\n",
      "172\n",
      "[ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7900 - accuracy: 0.8028\n",
      "173\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8086 - accuracy: 0.7956\n",
      "174\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7628 - accuracy: 0.8040\n",
      "175\n",
      "[ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7236 - accuracy: 0.8120\n",
      "176\n",
      "[ 0  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8381 - accuracy: 0.8021\n",
      "177\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8344 - accuracy: 0.7972\n",
      "178\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7514 - accuracy: 0.7945\n",
      "179\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.7480 - accuracy: 0.8045\n",
      "180\n",
      "[ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7680 - accuracy: 0.7997\n",
      "181\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7413 - accuracy: 0.8088\n",
      "182\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8200 - accuracy: 0.7936\n",
      "183\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7389 - accuracy: 0.8071\n",
      "184\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7519 - accuracy: 0.8063\n",
      "185\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 1s 3ms/step - loss: 0.6687 - accuracy: 0.8167\n",
      "186\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7565 - accuracy: 0.8103\n",
      "187\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8207 - accuracy: 0.8008\n",
      "188\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7503 - accuracy: 0.8089\n",
      "189\n",
      "[ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8695 - accuracy: 0.7960\n",
      "190\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7468 - accuracy: 0.8088\n",
      "191\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7334 - accuracy: 0.8140\n",
      "192\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7322 - accuracy: 0.8135\n",
      "193\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7481 - accuracy: 0.8124\n",
      "194\n",
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7802 - accuracy: 0.8079\n",
      "195\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.9314 - accuracy: 0.7850\n",
      "196\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.6772 - accuracy: 0.8180\n",
      "197\n",
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      " 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7042 - accuracy: 0.8148\n",
      "198\n",
      "[ 0  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.7033 - accuracy: 0.8135\n",
      "199\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29]\n",
      "556/556 [==============================] - 2s 3ms/step - loss: 0.8414 - accuracy: 0.7994\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "w_queue = deque([], maxlen=10)  # 创建一个长度为10的双向队列\n",
    "g_queue = deque([], maxlen=10)\n",
    "global_parameters =[]\n",
    "global_gradients=[]\n",
    "client_parameters=[]\n",
    "client_gradients =[]\n",
    "loss_3 =[]\n",
    "att_k = 3\n",
    "\n",
    "model_list =[]\n",
    "for i in range(30):\n",
    "    model = set_model()\n",
    "    model_list.append(model)\n",
    "print(type(model_list))\n",
    "\n",
    "model_list,global_parameters, global_gradients,client_parameters,client_gradients = federated_learning(model_list,x_train,y_train,att_k)\n",
    "model_list = fedavg(global_parameters,model_list)\n",
    "\n",
    "global_gradients_flattened =  flattened_g(global_gradients)\n",
    "global_parameters_flattened =  flattened_p(global_parameters)\n",
    "last_global_gradients_flattened = global_gradients_flattened\n",
    "last_global_parameters_flattened = global_parameters_flattened\n",
    "\n",
    "\n",
    "\n",
    "for i in range (10):\n",
    "    model_list,global_parameters, global_gradients,client_parameters,client_gradients = federated_learning(model_list,x_train,y_train,att_k)\n",
    "    model_list = fedavg(global_parameters,model_list)\n",
    "    global_gradients_flattened =  flattened_g(global_gradients)\n",
    "    global_parameters_flattened =  flattened_p(global_parameters)\n",
    "    w_queue.append(global_parameters_flattened- last_global_parameters_flattened)\n",
    "    g_queue.append(global_gradients_flattened- last_global_gradients_flattened)\n",
    "    #########################这里的代码有问题，for i in range(len(global_parameters))，应该是一个，记得修改\n",
    "    last_global_gradients_flattened = global_gradients_flattened\n",
    "    last_global_parameters_flattened = global_parameters_flattened\n",
    "    #############################v = para - last para\n",
    "    last_para = []\n",
    "    last_grad = []\n",
    "    for i in range (30):\n",
    "        last_grad.append(flattened_g(client_gradients[i])) \n",
    "        last_para.append(flattened_p(client_parameters[i])) \n",
    "\n",
    "        \n",
    "for i in range (200):\n",
    "    model_list,global_parameters, global_gradients,client_parameters,client_gradients = federated_learning(model_list,x_train,y_train,att_k)\n",
    "    para = []\n",
    "    grad = []\n",
    "    print(i)\n",
    "    for i in range (30):\n",
    "        grad.append(flattened_g(client_gradients[i])) \n",
    "        para.append(flattened_p(client_parameters[i]))\n",
    "    v = [current - last for current, last in zip(para, last_para)]\n",
    "    model_list,global_parameters, global_gradients = fedavg_2(client_parameters,client_gradients,v,w_queue,g_queue,grad)\n",
    "    global_gradients_flattened =  flattened_g(global_gradients)\n",
    "    global_parameters_flattened =  flattened_p(global_parameters)\n",
    "    w_queue.append(global_parameters_flattened- last_global_parameters_flattened)\n",
    "    g_queue.append(global_gradients_flattened- last_global_gradients_flattened)\n",
    "    #########################这里的代码有问题，for i in range(len(global_parameters))，应该是一个，记得修改\n",
    "    last_global_gradients_flattened = global_gradients_flattened\n",
    "    last_global_parameters_flattened = global_parameters_flattened\n",
    "    #############################v = para - last para\n",
    "    last_para = para\n",
    "    last_grad = grad \n",
    "    loss,accuracy = model_list[0].evaluate(x_test,y_test,verbose=1)\n",
    "    loss_3.append(loss)\n",
    "    scipy.io.savemat('loss_3.mat', mdict={'loss_3': loss_3,})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8971743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7619be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b07e97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c81875c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0ec81d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc6dd08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a01bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df0ef58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41439b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc7f961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f05b701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad605d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc68bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879996a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37243ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e31e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4ebd60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef67461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f15f1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b27310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef8c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee79d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef9e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73435809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0eeb52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65c8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbc1e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a78fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
